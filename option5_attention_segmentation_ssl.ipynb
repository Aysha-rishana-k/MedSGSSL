{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üì¶ Step 1: Import Libraries\n",
    "# ============================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cv2\n",
    "import random\n",
    "from scipy.ndimage import rotate as scipy_rotate\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìÅ Step 2: Download and Load Dataset\n",
    "# ============================================\n",
    "\n",
    "path = kagglehub.dataset_download(\"khanfashee/nih-chest-x-ray-14-224x224-resized\")\n",
    "BASE_PATH = Path(path)\n",
    "\n",
    "df_labels = pd.read_csv(BASE_PATH / \"Data_Entry_2017.csv\")\n",
    "images_dir = BASE_PATH / \"images-224\" / \"images-224\"\n",
    "df_labels[\"Image Path\"] = [str(images_dir / p) for p in df_labels[\"Image Index\"].values]\n",
    "\n",
    "DISEASE_CATEGORIES = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass',\n",
    "    'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia'\n",
    "]\n",
    "\n",
    "for disease in DISEASE_CATEGORIES:\n",
    "    df_labels[disease] = df_labels['Finding Labels'].apply(lambda x: 1 if disease in x else 0)\n",
    "\n",
    "sample_paths = df_labels['Image Path'].sample(200, random_state=42).values\n",
    "missing = [p for p in sample_paths if not os.path.exists(p)]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"‚ùå Missing {len(missing)} images!\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_labels):,} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa802e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ‚öôÔ∏è Step 3: Configuration\n",
    "# ============================================\n",
    "\n",
    "class Config:\n",
    "    img_size = 224\n",
    "    feat_dim = 256\n",
    "    proj_dim = 128\n",
    "    \n",
    "    batch_size = 64\n",
    "    pretrain_epochs = 50\n",
    "    finetune_epochs = 30\n",
    "    lr_pretrain = 1e-3\n",
    "    lr_finetune = 1e-4\n",
    "    temperature = 0.1\n",
    "    \n",
    "    num_workers = 4\n",
    "    use_subset = False\n",
    "    \n",
    "    # Attention parameters\n",
    "    num_regions = 6\n",
    "    attention_layers = [2, 3, 4]\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   Device: {cfg.device}\")\n",
    "print(f\"   Num regions: {cfg.num_regions}\")\n",
    "print(f\"   Attention layers: {cfg.attention_layers}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26fdf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üíæ Step 3.5: Checkpoint & Resume Configuration\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "OPTION_NAME = \"option5\"\n",
    "\n",
    "# ===== RESUME CONFIGURATION =====\n",
    "CHECKPOINT_DATASET_NAME = \"chest-xray-ssl-checkpoints\"\n",
    "RESUME_SSL_PRETRAINING = False\n",
    "RESUME_FINETUNING = False\n",
    "SSL_CHECKPOINT_FILE = \"latest\"\n",
    "FINETUNE_CHECKPOINT_FILE = \"latest\"\n",
    "\n",
    "IN_KAGGLE = os.path.exists('/kaggle')\n",
    "IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/chest_xray_ssl'\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "    PREV_CHECKPOINT_DIR = f'/kaggle/input/{CHECKPOINT_DATASET_NAME}'\n",
    "    if os.path.exists(PREV_CHECKPOINT_DIR):\n",
    "        print(f\"‚úÖ Found checkpoints at: {PREV_CHECKPOINT_DIR}\")\n",
    "        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "        for f in os.listdir(PREV_CHECKPOINT_DIR):\n",
    "            if f.endswith('.pth'):\n",
    "                src, dst = os.path.join(PREV_CHECKPOINT_DIR, f), os.path.join(CHECKPOINT_DIR, f)\n",
    "                if not os.path.exists(dst): shutil.copy2(src, dst)\n",
    "elif not IN_COLAB:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    filepath = os.path.join(CHECKPOINT_DIR, filename)\n",
    "    state['saved_at'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    torch.save(state, filepath)\n",
    "    print(f\"üíæ Saved: {filename}\")\n",
    "    if IN_KAGGLE: torch.save(state, f'/kaggle/working/{filename}')\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    filepath = os.path.join(CHECKPOINT_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=cfg.device)\n",
    "        print(f\"‚úÖ Loaded: {filename}\")\n",
    "        return checkpoint\n",
    "    return None\n",
    "\n",
    "def find_latest_checkpoint(prefix):\n",
    "    if not os.path.exists(CHECKPOINT_DIR): return None\n",
    "    latest = f'{prefix}_latest.pth'\n",
    "    if os.path.exists(os.path.join(CHECKPOINT_DIR, latest)): return latest\n",
    "    import re\n",
    "    pattern = re.compile(rf'{prefix}_epoch(\\d+)\\.pth')\n",
    "    max_epoch, best = -1, None\n",
    "    for f in os.listdir(CHECKPOINT_DIR):\n",
    "        m = pattern.match(f)\n",
    "        if m and int(m.group(1)) > max_epoch: max_epoch, best = int(m.group(1)), f\n",
    "    return best\n",
    "\n",
    "print(f\"üîß Environment: {'Kaggle' if IN_KAGGLE else 'Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"üìÇ Checkpoint dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ü´Å Step 4: Multi-Region Segmentation\n",
    "# ============================================\n",
    "\n",
    "def multi_region_segmentation(image):\n",
    "    \"\"\"\n",
    "    Divide image into 6 anatomical regions:\n",
    "    - Upper-Left, Upper-Right\n",
    "    - Middle-Left, Middle-Right\n",
    "    - Lower-Left, Lower-Right\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3 and image.shape[0] == 1:\n",
    "        image = image[0]\n",
    "    \n",
    "    h, w = image.shape\n",
    "    regions = {}\n",
    "    \n",
    "    h_third = h // 3\n",
    "    w_half = w // 2\n",
    "    \n",
    "    region_names = [\n",
    "        'upper_left', 'upper_right',\n",
    "        'middle_left', 'middle_right',\n",
    "        'lower_left', 'lower_right'\n",
    "    ]\n",
    "    \n",
    "    for i, name in enumerate(region_names):\n",
    "        regions[name] = np.zeros_like(image)\n",
    "    \n",
    "    regions['upper_left'][0:h_third, 0:w_half] = 1\n",
    "    regions['upper_right'][0:h_third, w_half:w] = 1\n",
    "    regions['middle_left'][h_third:2*h_third, 0:w_half] = 1\n",
    "    regions['middle_right'][h_third:2*h_third, w_half:w] = 1\n",
    "    regions['lower_left'][2*h_third:h, 0:w_half] = 1\n",
    "    regions['lower_right'][2*h_third:h, w_half:w] = 1\n",
    "    \n",
    "    return regions\n",
    "\n",
    "\n",
    "def visualize_regions(image):\n",
    "    \"\"\"Visualize regions with different colors\"\"\"\n",
    "    if len(image.shape) == 3 and image.shape[0] == 1:\n",
    "        image = image[0]\n",
    "    \n",
    "    regions = multi_region_segmentation(image)\n",
    "    h, w = image.shape\n",
    "    colored = np.zeros((h, w, 3))\n",
    "    \n",
    "    colors = {\n",
    "        'upper_left': [1, 0, 0],\n",
    "        'upper_right': [0, 1, 0],\n",
    "        'middle_left': [0, 0, 1],\n",
    "        'middle_right': [1, 1, 0],\n",
    "        'lower_left': [1, 0, 1],\n",
    "        'lower_right': [0, 1, 1]\n",
    "    }\n",
    "    \n",
    "    for region_name, color in colors.items():\n",
    "        mask = regions[region_name] > 0.5\n",
    "        colored[mask] = np.array(color)\n",
    "    \n",
    "    return colored\n",
    "\n",
    "print(\"‚úÖ Region segmentation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üëÅÔ∏è Step 5: Visualize Region Segmentation\n",
    "# ============================================\n",
    "\n",
    "sample_indices = [0, 100, 500]\n",
    "fig, axes = plt.subplots(len(sample_indices), 3, figsize=(14, 4*len(sample_indices)))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    img_path = df_labels.iloc[idx]['Image Path']\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    img = img.resize((cfg.img_size, cfg.img_size), Image.LANCZOS)\n",
    "    img_np = np.array(img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    axes[i, 0].imshow(img_np, cmap='gray')\n",
    "    axes[i, 0].set_title('Original Image')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Region masks\n",
    "    regions = multi_region_segmentation(img_np)\n",
    "    region_vis = visualize_regions(img_np)\n",
    "    axes[i, 1].imshow(region_vis)\n",
    "    axes[i, 1].set_title('6 Anatomical Regions (Color-Coded)')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = np.stack([img_np, img_np, img_np], axis=2)\n",
    "    overlay = 0.7 * overlay + 0.3 * region_vis\n",
    "    axes[i, 2].imshow(overlay)\n",
    "    axes[i, 2].set_title('Overlay')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Option 5: Multi-Region Attention Guidance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('option5_region_attention.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Region visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a72a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üîÑ Step 6: Context-Aware Augmentation\n",
    "# ============================================\n",
    "\n",
    "def apply_augmentation(image):\n",
    "    \"\"\"Standard augmentation\"\"\"\n",
    "    # Rotation\n",
    "    if random.random() > 0.5:\n",
    "        angle = random.uniform(-10, 10)\n",
    "        image = scipy_rotate(image, angle, reshape=False, mode='nearest')\n",
    "    \n",
    "    # Horizontal flip\n",
    "    if random.random() > 0.5:\n",
    "        image = np.fliplr(image)\n",
    "    \n",
    "    # Brightness\n",
    "    if random.random() > 0.5:\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        image = np.clip(image * factor, 0, 1)\n",
    "    \n",
    "    # Contrast\n",
    "    if random.random() > 0.5:\n",
    "        mean = image.mean()\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        image = np.clip((image - mean) * factor + mean, 0, 1)\n",
    "    \n",
    "    # Gaussian noise\n",
    "    if random.random() > 0.6:\n",
    "        noise = np.random.normal(0, 0.02, image.shape)\n",
    "        image = np.clip(image + noise, 0, 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "print(\"‚úÖ Augmentation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üì¶ Step 7: Dataset Classes\n",
    "# ============================================\n",
    "\n",
    "class AttentionGuidedDataset(Dataset):\n",
    "    \"\"\"SSL dataset with region attention guidance\"\"\"\n",
    "    \n",
    "    def __init__(self, df, img_size=224):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        sample_paths = self.df['Image Path'].sample(min(200, len(self.df)), random_state=42).values\n",
    "        missing = [p for p in sample_paths if not os.path.exists(p)]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(f\"‚ùå Missing {len(missing)} images!\")\n",
    "        \n",
    "        print(f\"üì¶ AttentionGuidedDataset: {len(self.df)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['Image Path']\n",
    "        \n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img = img.resize((self.img_size, self.img_size), Image.LANCZOS)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Get region masks\n",
    "        regions = multi_region_segmentation(img)\n",
    "        region_masks = np.stack([regions[k] for k in sorted(regions.keys())], axis=0)\n",
    "        \n",
    "        # Two augmented views\n",
    "        view1 = apply_augmentation(img.copy())\n",
    "        view2 = apply_augmentation(img.copy())\n",
    "        \n",
    "        view1 = torch.tensor(view1[None, ...], dtype=torch.float32)\n",
    "        view2 = torch.tensor(view2[None, ...], dtype=torch.float32)\n",
    "        region_masks = torch.tensor(region_masks, dtype=torch.float32)\n",
    "        \n",
    "        return view1, view2, region_masks\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    \"\"\"Classification dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, df, disease_categories, img_size=224):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.disease_categories = disease_categories\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        print(f\"üì¶ ClassificationDataset: {len(self.df)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img = Image.open(row['Image Path']).convert('L')\n",
    "        img = img.resize((self.img_size, self.img_size), Image.LANCZOS)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        img = torch.tensor(img[None, ...], dtype=torch.float32)\n",
    "        \n",
    "        labels = torch.tensor([row[d] for d in self.disease_categories], dtype=torch.float32)\n",
    "        \n",
    "        return img, labels\n",
    "\n",
    "print(\"‚úÖ Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e89843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üèóÔ∏è Step 8: Attention-Guided Encoder\n",
    "# ============================================\n",
    "\n",
    "def conv_block(in_c, out_c, kernel=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel, stride, padding),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def residual_block(channels):\n",
    "    return nn.Sequential(\n",
    "        conv_block(channels, channels),\n",
    "        conv_block(channels, channels)\n",
    "    )\n",
    "\n",
    "class RegionAttention(nn.Module):\n",
    "    \"\"\"Region-specific attention module\"\"\"\n",
    "    def __init__(self, in_channels, num_regions=6):\n",
    "        super().__init__()\n",
    "        self.num_regions = num_regions\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, num_regions),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, region_masks):\n",
    "        \"\"\"Apply region-aware attention\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        attention_weights = self.attention(x)\n",
    "        \n",
    "        # Resize region masks to match feature map size\n",
    "        h, w = x.shape[2:]\n",
    "        region_masks_resized = F.interpolate(\n",
    "            region_masks, size=(h, w), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Apply region-aware weighting\n",
    "        weighted_x = x.clone()\n",
    "        for i in range(self.num_regions):\n",
    "            region_mask = region_masks_resized[:, i:i+1, :, :]\n",
    "            weight = attention_weights[:, i:i+1].view(batch_size, 1, 1, 1)\n",
    "            weighted_x = weighted_x + weight * region_mask * x\n",
    "        \n",
    "        return weighted_x / self.num_regions\n",
    "\n",
    "\n",
    "class AttentionEncoder(nn.Module):\n",
    "    \"\"\"Encoder with segmentation-guided attention\"\"\"\n",
    "    def __init__(self, in_channels=1, feat_dim=256, num_regions=6):\n",
    "        super().__init__()\n",
    "        self.num_regions = num_regions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(in_channels, 64), residual_block(64), nn.MaxPool2d(2),\n",
    "            conv_block(64, 128), residual_block(128), nn.MaxPool2d(2),\n",
    "            conv_block(128, 256), residual_block(256), residual_block(256), nn.MaxPool2d(2),\n",
    "            conv_block(256, 512), residual_block(512), residual_block(512), nn.MaxPool2d(2),\n",
    "            conv_block(512, 512), residual_block(512), nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        self.region_attention_1 = RegionAttention(128, num_regions)\n",
    "        self.region_attention_2 = RegionAttention(256, num_regions)\n",
    "        self.region_attention_3 = RegionAttention(512, num_regions)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, feat_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, region_masks=None):\n",
    "        # If no attention guidance, use standard encoder\n",
    "        if region_masks is None:\n",
    "            x = self.features(x)\n",
    "            return self.fc(x.view(x.size(0), -1))\n",
    "        \n",
    "        # Apply attention at intermediate layers\n",
    "        x = self.features[0:3](x)\n",
    "        x = self.region_attention_1(x, region_masks)\n",
    "        \n",
    "        x = self.features[3:6](x)\n",
    "        x = self.region_attention_2(x, region_masks)\n",
    "        \n",
    "        x = self.features[6:11](x)\n",
    "        x = self.region_attention_3(x, region_masks)\n",
    "        \n",
    "        x = self.features[11:](x)\n",
    "        return self.fc(x.view(x.size(0), -1))\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, feat_dim=256, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_dim, feat_dim), nn.BatchNorm1d(feat_dim), nn.ReLU(),\n",
    "            nn.Linear(feat_dim, proj_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, feat_dim=256, img_size=224):\n",
    "        super().__init__()\n",
    "        self.init_size = img_size // 32\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256 * self.init_size * self.init_size), nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 4, 2, 1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        return self.decoder(x.view(z.size(0), 256, self.init_size, self.init_size))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feat_dim=256, num_classes=14):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "encoder = AttentionEncoder(feat_dim=cfg.feat_dim, num_regions=cfg.num_regions).to(cfg.device)\n",
    "proj_head = ProjectionHead(cfg.feat_dim, cfg.proj_dim).to(cfg.device)\n",
    "decoder = Decoder(cfg.feat_dim, cfg.img_size).to(cfg.device)\n",
    "\n",
    "total_params = sum(p.numel() for m in [encoder, proj_head, decoder] for p in m.parameters())\n",
    "print(f\"‚úÖ Attention-Guided Models initialized ({total_params:,} parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üî• Step 9: Loss Functions\n",
    "# ============================================\n",
    "\n",
    "def nt_xent_loss(z1, z2, temperature=0.1):\n",
    "    device = z1.device\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    batch_size = z1.shape[0]\n",
    "    representations = torch.cat([z1, z2], dim=0)\n",
    "    similarity = torch.matmul(representations, representations.T) / temperature\n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool, device=device)\n",
    "    similarity = similarity.masked_fill(mask, -float('inf'))\n",
    "    labels = torch.cat([torch.arange(batch_size) + batch_size,\n",
    "                        torch.arange(batch_size)]).to(device)\n",
    "    return F.cross_entropy(similarity, labels)\n",
    "\n",
    "print(\"‚úÖ Loss functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b00b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìä Step 10: Create Data Loaders\n",
    "# ============================================\n",
    "\n",
    "df_shuffled = df_labels.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_size = int(0.8 * len(df_shuffled))\n",
    "train_df = df_shuffled[:train_size]\n",
    "val_df = df_shuffled[train_size:]\n",
    "\n",
    "train_pretrain_ds = AttentionGuidedDataset(train_df, img_size=cfg.img_size)\n",
    "train_class_ds = ClassificationDataset(train_df, DISEASE_CATEGORIES, cfg.img_size)\n",
    "val_class_ds = ClassificationDataset(val_df, DISEASE_CATEGORIES, cfg.img_size)\n",
    "\n",
    "pretrain_loader = DataLoader(\n",
    "    train_pretrain_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=cfg.num_workers, pin_memory=True, drop_last=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_class_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=cfg.num_workers, pin_memory=True, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_class_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=cfg.num_workers, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üöÄ Step 11: Attention-Guided SSL Pretraining\n",
    "# ============================================\n",
    "\n",
    "optimizer_ssl = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(proj_head.parameters()) + list(decoder.parameters()),\n",
    "    lr=cfg.lr_pretrain, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "ssl_history = {'loss': [], 'contrastive': [], 'reconstruction': []}\n",
    "START_EPOCH = 1\n",
    "\n",
    "if RESUME_SSL_PRETRAINING:\n",
    "    ckpt_file = find_latest_checkpoint(f'{OPTION_NAME}_ssl') if SSL_CHECKPOINT_FILE == \"latest\" else SSL_CHECKPOINT_FILE\n",
    "    if ckpt_file:\n",
    "        checkpoint = load_checkpoint(ckpt_file)\n",
    "        if checkpoint:\n",
    "            encoder.load_state_dict(checkpoint['encoder'])\n",
    "            proj_head.load_state_dict(checkpoint['proj_head'])\n",
    "            decoder.load_state_dict(checkpoint['decoder'])\n",
    "            if 'optimizer' in checkpoint: optimizer_ssl.load_state_dict(checkpoint['optimizer'])\n",
    "            ssl_history = checkpoint.get('ssl_history', ssl_history)\n",
    "            START_EPOCH = checkpoint['epoch'] + 1\n",
    "            print(f\"üîÑ Resuming from epoch {START_EPOCH}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No checkpoint found. Starting fresh.\")\n",
    "\n",
    "if START_EPOCH > cfg.pretrain_epochs:\n",
    "    print(f\"‚úÖ SSL Pretraining already complete\")\n",
    "else:\n",
    "    print(f\"\\nüß† Starting Option 5: Attention-Guided SSL Pretraining\")\n",
    "    print(f\"   Epochs: {START_EPOCH} ‚Üí {cfg.pretrain_epochs}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(START_EPOCH, cfg.pretrain_epochs + 1):\n",
    "        encoder.train()\n",
    "        proj_head.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        total_loss = total_cont = total_recon = 0\n",
    "        \n",
    "        pbar = tqdm(pretrain_loader, desc=f\"Epoch {epoch}/{cfg.pretrain_epochs}\")\n",
    "        for view1, view2, region_masks in pbar:\n",
    "            view1 = view1.to(cfg.device)\n",
    "            view2 = view2.to(cfg.device)\n",
    "            region_masks = region_masks.to(cfg.device)\n",
    "            \n",
    "            optimizer_ssl.zero_grad()\n",
    "            \n",
    "            z1 = encoder(view1, region_masks)\n",
    "            z2 = encoder(view2, region_masks)\n",
    "            \n",
    "            p1, p2 = proj_head(z1), proj_head(z2)\n",
    "            cont_loss = nt_xent_loss(p1, p2, cfg.temperature)\n",
    "            \n",
    "            rec1, rec2 = decoder(z1), decoder(z2)\n",
    "            recon_loss = (F.mse_loss(rec1, view1) + F.mse_loss(rec2, view2)) / 2\n",
    "            \n",
    "            loss = cont_loss + 0.5 * recon_loss\n",
    "            loss.backward()\n",
    "            optimizer_ssl.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_cont += cont_loss.item()\n",
    "            total_recon += recon_loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        n = len(pretrain_loader)\n",
    "        ssl_history['loss'].append(total_loss / n)\n",
    "        ssl_history['contrastive'].append(total_cont / n)\n",
    "        ssl_history['reconstruction'].append(total_recon / n)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss={total_loss/n:.4f}, Cont={total_cont/n:.4f}, Recon={total_recon/n:.4f}\")\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch, 'encoder': encoder.state_dict(),\n",
    "            'proj_head': proj_head.state_dict(), 'decoder': decoder.state_dict(),\n",
    "            'optimizer': optimizer_ssl.state_dict(), 'ssl_history': ssl_history,\n",
    "        }, f'{OPTION_NAME}_ssl_latest.pth')\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch, 'encoder': encoder.state_dict(),\n",
    "            'proj_head': proj_head.state_dict(), 'decoder': decoder.state_dict(),\n",
    "            'ssl_history': ssl_history,\n",
    "        }, f'{OPTION_NAME}_ssl_epoch{epoch}.pth')\n",
    "    \n",
    "    print(\"\\n‚úÖ Attention-Guided SSL Pretraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìà Step 12: Plot SSL Curves & Save Model\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(ssl_history['loss'], 'b-', linewidth=2)\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(ssl_history['contrastive'], 'r-', linewidth=2)\n",
    "axes[1].set_title('Contrastive Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(ssl_history['reconstruction'], 'g-', linewidth=2)\n",
    "axes[2].set_title('Reconstruction Loss')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Option 5: Attention-Guided SSL Training', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('option5_ssl_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "torch.save({\n",
    "    'encoder': encoder.state_dict(),\n",
    "    'proj_head': proj_head.state_dict(),\n",
    "    'decoder': decoder.state_dict(),\n",
    "}, 'option5_ssl_pretrained.pth')\n",
    "\n",
    "print(\"üíæ Pretrained model saved: option5_ssl_pretrained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87185215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üéØ Step 13-15: Fine-tuning, Evaluation, & Summary\n",
    "# ============================================\n",
    "\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "encoder.eval()\n",
    "\n",
    "classifier = Classifier(cfg.feat_dim, len(DISEASE_CATEGORIES)).to(cfg.device)\n",
    "\n",
    "pos_counts = train_df[DISEASE_CATEGORIES].sum().values\n",
    "neg_counts = len(train_df) - pos_counts\n",
    "pos_weights = torch.tensor(neg_counts / (pos_counts + 1e-6), dtype=torch.float32).to(cfg.device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "optimizer_ft = torch.optim.Adam(classifier.parameters(), lr=cfg.lr_finetune, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'max', patience=5, factor=0.5)\n",
    "\n",
    "finetune_history = {'train_loss': [], 'train_auc': [], 'val_loss': [], 'val_auc': []}\n",
    "best_val_auc = 0\n",
    "FINETUNE_START_EPOCH = 1\n",
    "\n",
    "if RESUME_FINETUNING:\n",
    "    ckpt_file = find_latest_checkpoint(f'{OPTION_NAME}_finetune') if FINETUNE_CHECKPOINT_FILE == \"latest\" else FINETUNE_CHECKPOINT_FILE\n",
    "    if ckpt_file:\n",
    "        checkpoint = load_checkpoint(ckpt_file)\n",
    "        if checkpoint:\n",
    "            classifier.load_state_dict(checkpoint['classifier'])\n",
    "            if 'optimizer' in checkpoint: optimizer_ft.load_state_dict(checkpoint['optimizer'])\n",
    "            if 'scheduler' in checkpoint: scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "            finetune_history = checkpoint.get('finetune_history', finetune_history)\n",
    "            best_val_auc = checkpoint.get('best_val_auc', 0)\n",
    "            FINETUNE_START_EPOCH = checkpoint['epoch'] + 1\n",
    "            print(f\"üîÑ Resuming fine-tuning from epoch {FINETUNE_START_EPOCH}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No checkpoint found. Starting fresh.\")\n",
    "\n",
    "if FINETUNE_START_EPOCH > cfg.finetune_epochs:\n",
    "    print(f\"‚úÖ Fine-tuning already complete\")\n",
    "else:\n",
    "    print(\"üéØ Starting Fine-tuning\")\n",
    "    print(f\"   Epochs: {FINETUNE_START_EPOCH} ‚Üí {cfg.finetune_epochs}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for epoch in range(FINETUNE_START_EPOCH, cfg.finetune_epochs + 1):\n",
    "        classifier.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "        \n",
    "        for images, targets in tqdm(train_loader, desc=f\"Train {epoch}/{cfg.finetune_epochs}\"):\n",
    "            images, targets = images.to(cfg.device), targets.to(cfg.device)\n",
    "            optimizer_ft.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                features = encoder(images)\n",
    "            logits = classifier(features)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer_ft.step()\n",
    "            train_loss += loss.item()\n",
    "            train_preds.append(torch.sigmoid(logits).detach().cpu())\n",
    "            train_targets.append(targets.cpu())\n",
    "        \n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images, targets = images.to(cfg.device), targets.to(cfg.device)\n",
    "                features = encoder(images)\n",
    "                logits = classifier(features)\n",
    "                val_loss += criterion(logits, targets).item()\n",
    "                val_preds.append(torch.sigmoid(logits).cpu())\n",
    "                val_targets.append(targets.cpu())\n",
    "        \n",
    "        train_preds = torch.cat(train_preds).numpy()\n",
    "        train_targets = torch.cat(train_targets).numpy()\n",
    "        val_preds = torch.cat(val_preds).numpy()\n",
    "        val_targets = torch.cat(val_targets).numpy()\n",
    "        \n",
    "        train_auc = np.mean([roc_auc_score(train_targets[:, i], train_preds[:, i]) \n",
    "                             for i in range(len(DISEASE_CATEGORIES)) \n",
    "                             if len(np.unique(train_targets[:, i])) > 1])\n",
    "        val_auc = np.mean([roc_auc_score(val_targets[:, i], val_preds[:, i]) \n",
    "                           for i in range(len(DISEASE_CATEGORIES)) \n",
    "                           if len(np.unique(val_targets[:, i])) > 1])\n",
    "        \n",
    "        finetune_history['train_loss'].append(train_loss / len(train_loader))\n",
    "        finetune_history['train_auc'].append(train_auc)\n",
    "        finetune_history['val_loss'].append(val_loss / len(val_loader))\n",
    "        finetune_history['val_auc'].append(val_auc)\n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train AUC={train_auc:.4f}, Val AUC={val_auc:.4f}\")\n",
    "        \n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            torch.save({'encoder': encoder.state_dict(), 'classifier': classifier.state_dict()}, \n",
    "                       f'{OPTION_NAME}_best_model.pth')\n",
    "            print(f\"  ‚úÖ Best model saved! Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch, 'classifier': classifier.state_dict(),\n",
    "            'optimizer': optimizer_ft.state_dict(), 'scheduler': scheduler.state_dict(),\n",
    "            'finetune_history': finetune_history, 'best_val_auc': best_val_auc,\n",
    "        }, f'{OPTION_NAME}_finetune_latest.pth')\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch, 'classifier': classifier.state_dict(),\n",
    "            'finetune_history': finetune_history, 'best_val_auc': best_val_auc,\n",
    "        }, f'{OPTION_NAME}_finetune_epoch{epoch}.pth')\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ac044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìä Step 16: Final Evaluation & Results\n",
    "# ============================================\n",
    "\n",
    "checkpoint = torch.load(f'{OPTION_NAME}_best_model.pth')\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "classifier.load_state_dict(checkpoint['classifier'])\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(cfg.device)\n",
    "        features = encoder(images)\n",
    "        logits = classifier(features)\n",
    "        all_preds.append(torch.sigmoid(logits).cpu())\n",
    "        all_targets.append(targets)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "print(\"\\nüìä Per-Disease AUC Scores:\")\n",
    "print(\"=\" * 40)\n",
    "auc_scores = []\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(all_targets[:, i])) > 1:\n",
    "        auc = roc_auc_score(all_targets[:, i], all_preds[:, i])\n",
    "        auc_scores.append((disease, auc))\n",
    "        print(f\"{disease:20s}: {auc:.4f}\")\n",
    "\n",
    "mean_auc = np.mean([a for _, a in auc_scores])\n",
    "print(f\"\\n{'Mean AUC':20s}: {mean_auc:.4f}\")\n",
    "\n",
    "auc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "diseases, aucs = zip(*auc_scores)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['green' if a >= 0.7 else 'orange' if a >= 0.6 else 'red' for a in aucs]\n",
    "plt.barh(diseases, aucs, color=colors, alpha=0.8)\n",
    "plt.axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "plt.axvline(mean_auc, color='blue', linestyle='--', alpha=0.7, label=f'Mean: {mean_auc:.3f}')\n",
    "plt.xlabel('AUC Score')\n",
    "plt.title('Option 5: Per-Disease AUC Performance (Attention-Guided)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OPTION_NAME}_auc_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üß† OPTION 5: SEGMENTATION-GUIDED ATTENTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Method: Region-Specific Attention in Encoder Architecture\")\n",
    "print(f\"\\nKey Advantages:\")\n",
    "print(f\"  ‚úÖ Learnable region-specific attention weights\")\n",
    "print(f\"  ‚úÖ Network learns which regions matter for SSL\")\n",
    "print(f\"  ‚úÖ Combines supervised segmentation with unsupervised SSL\")\n",
    "print(f\"  ‚úÖ More flexible than hard cropping\")\n",
    "print(f\"  ‚úÖ Emergent focus on pathological patterns\")\n",
    "print(f\"\\nüèÜ Final Mean AUC: {mean_auc:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
