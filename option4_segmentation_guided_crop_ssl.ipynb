{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üì¶ Step 1: Import Libraries\n",
    "# ============================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cv2\n",
    "import random\n",
    "from scipy.ndimage import rotate as scipy_rotate\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed938f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìÅ Step 2: Download and Load Dataset\n",
    "# ============================================\n",
    "\n",
    "path = kagglehub.dataset_download(\"khanfashee/nih-chest-x-ray-14-224x224-resized\")\n",
    "BASE_PATH = Path(path)\n",
    "\n",
    "df_labels = pd.read_csv(BASE_PATH / \"Data_Entry_2017.csv\")\n",
    "images_dir = BASE_PATH / \"images-224\" / \"images-224\"\n",
    "df_labels[\"Image Path\"] = [str(images_dir / p) for p in df_labels[\"Image Index\"].values]\n",
    "\n",
    "DISEASE_CATEGORIES = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass',\n",
    "    'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia'\n",
    "]\n",
    "\n",
    "for disease in DISEASE_CATEGORIES:\n",
    "    df_labels[disease] = df_labels['Finding Labels'].apply(lambda x: 1 if disease in x else 0)\n",
    "\n",
    "sample_paths = df_labels['Image Path'].sample(200, random_state=42).values\n",
    "missing = [p for p in sample_paths if not os.path.exists(p)]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"‚ùå Missing {len(missing)} images!\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_labels):,} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ‚öôÔ∏è Step 3: Configuration\n",
    "# ============================================\n",
    "\n",
    "class Config:\n",
    "    img_size = 224\n",
    "    feat_dim = 256\n",
    "    proj_dim = 128\n",
    "    \n",
    "    batch_size = 64\n",
    "    pretrain_epochs = 50\n",
    "    finetune_epochs = 30\n",
    "    lr_pretrain = 1e-3\n",
    "    lr_finetune = 1e-4\n",
    "    temperature = 0.1\n",
    "    \n",
    "    num_workers = 4\n",
    "    use_subset = False\n",
    "    \n",
    "    # Cropping parameters\n",
    "    crop_padding = 20\n",
    "    min_crop_ratio = 0.5\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   Device: {cfg.device}\")\n",
    "print(f\"   Crop padding: {cfg.crop_padding}px\")\n",
    "print(f\"   Min crop ratio: {cfg.min_crop_ratio:.1%}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01358026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üíæ Step 3.5: Checkpoint & Resume Configuration\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "OPTION_NAME = \"option4\"\n",
    "\n",
    "# ===== RESUME CONFIGURATION =====\n",
    "CHECKPOINT_DATASET_NAME = \"chest-xray-ssl-checkpoints\"\n",
    "RESUME_SSL_PRETRAINING = False\n",
    "RESUME_FINETUNING = False\n",
    "SSL_CHECKPOINT_FILE = \"latest\"\n",
    "FINETUNE_CHECKPOINT_FILE = \"latest\"\n",
    "\n",
    "IN_KAGGLE = os.path.exists('/kaggle')\n",
    "IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/chest_xray_ssl'\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "    PREV_CHECKPOINT_DIR = f'/kaggle/input/{CHECKPOINT_DATASET_NAME}'\n",
    "    if os.path.exists(PREV_CHECKPOINT_DIR):\n",
    "        print(f\"‚úÖ Found checkpoints at: {PREV_CHECKPOINT_DIR}\")\n",
    "        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "        for f in os.listdir(PREV_CHECKPOINT_DIR):\n",
    "            if f.endswith('.pth'):\n",
    "                src, dst = os.path.join(PREV_CHECKPOINT_DIR, f), os.path.join(CHECKPOINT_DIR, f)\n",
    "                if not os.path.exists(dst): shutil.copy2(src, dst)\n",
    "elif not IN_COLAB:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    filepath = os.path.join(CHECKPOINT_DIR, filename)\n",
    "    state['saved_at'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    torch.save(state, filepath)\n",
    "    print(f\"üíæ Saved: {filename}\")\n",
    "    if IN_KAGGLE: torch.save(state, f'/kaggle/working/{filename}')\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    filepath = os.path.join(CHECKPOINT_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=cfg.device)\n",
    "        print(f\"‚úÖ Loaded: {filename}\")\n",
    "        return checkpoint\n",
    "    return None\n",
    "\n",
    "def find_latest_checkpoint(prefix):\n",
    "    if not os.path.exists(CHECKPOINT_DIR): return None\n",
    "    latest = f'{prefix}_latest.pth'\n",
    "    if os.path.exists(os.path.join(CHECKPOINT_DIR, latest)): return latest\n",
    "    import re\n",
    "    pattern = re.compile(rf'{prefix}_epoch(\\d+)\\.pth')\n",
    "    max_epoch, best = -1, None\n",
    "    for f in os.listdir(CHECKPOINT_DIR):\n",
    "        m = pattern.match(f)\n",
    "        if m and int(m.group(1)) > max_epoch: max_epoch, best = int(m.group(1)), f\n",
    "    return best\n",
    "\n",
    "print(f\"üîß Environment: {'Kaggle' if IN_KAGGLE else 'Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"üìÇ Checkpoint dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da237c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ü´Å Step 4: Lung Segmentation & Cropping\n",
    "# ============================================\n",
    "\n",
    "def segment_lungs(image):\n",
    "    \"\"\"Segment lung fields using Otsu + morphology\"\"\"\n",
    "    if len(image.shape) == 3 and image.shape[0] == 1:\n",
    "        image = image[0]\n",
    "    \n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    _, binary = cv2.threshold(img_uint8, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    binary = 255 - binary\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
    "    lung_mask = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    lung_mask = cv2.morphologyEx(lung_mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    return lung_mask.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def get_lung_bounding_box(mask):\n",
    "    \"\"\"Get bounding box of lung fields\"\"\"\n",
    "    coords = np.argwhere(mask > 0.5)\n",
    "    \n",
    "    if len(coords) == 0:\n",
    "        return None\n",
    "    \n",
    "    y_min, x_min = coords.min(axis=0)\n",
    "    y_max, x_max = coords.max(axis=0)\n",
    "    \n",
    "    return y_min, y_max, x_min, x_max\n",
    "\n",
    "\n",
    "def context_aware_crop(image, mask, padding=20, min_crop_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Crop image to lung bounding box with padding\n",
    "    Preserves anatomical context while removing irrelevant background\n",
    "    \"\"\"\n",
    "    bbox = get_lung_bounding_box(mask)\n",
    "    \n",
    "    if bbox is None:\n",
    "        return image, mask\n",
    "    \n",
    "    y_min, y_max, x_min, x_max = bbox\n",
    "    h, w = image.shape\n",
    "    \n",
    "    y_min = max(0, y_min - padding)\n",
    "    x_min = max(0, x_min - padding)\n",
    "    y_max = min(h, y_max + padding)\n",
    "    x_max = min(w, x_max + padding)\n",
    "    \n",
    "    crop_h = y_max - y_min\n",
    "    crop_w = x_max - x_min\n",
    "    \n",
    "    if crop_h < h * min_crop_ratio or crop_w < w * min_crop_ratio:\n",
    "        return image, mask\n",
    "    \n",
    "    img_cropped = image[y_min:y_max, x_min:x_max]\n",
    "    mask_cropped = mask[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    return img_cropped, mask_cropped\n",
    "\n",
    "print(\"‚úÖ Lung segmentation & cropping functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üëÅÔ∏è Step 5: Visualize Smart Cropping\n",
    "# ============================================\n",
    "\n",
    "sample_indices = [0, 100, 500]\n",
    "fig, axes = plt.subplots(len(sample_indices), 3, figsize=(14, 4*len(sample_indices)))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    img_path = df_labels.iloc[idx]['Image Path']\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    img = img.resize((cfg.img_size, cfg.img_size), Image.LANCZOS)\n",
    "    img_np = np.array(img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    # Segment and crop\n",
    "    lung_mask = segment_lungs(img_np)\n",
    "    img_cropped, mask_cropped = context_aware_crop(img_np, lung_mask, cfg.crop_padding, cfg.min_crop_ratio)\n",
    "    \n",
    "    # Resize back\n",
    "    img_resized = cv2.resize(img_cropped, (cfg.img_size, cfg.img_size))\n",
    "    \n",
    "    # Plot original with bounding box\n",
    "    axes[i, 0].imshow(img_np, cmap='gray')\n",
    "    bbox = get_lung_bounding_box(lung_mask)\n",
    "    if bbox:\n",
    "        y_min, y_max, x_min, x_max = bbox\n",
    "        rect = plt.Rectangle((x_min-cfg.crop_padding, y_min-cfg.crop_padding),\n",
    "                             x_max-x_min+2*cfg.crop_padding, y_max-y_min+2*cfg.crop_padding,\n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "        axes[i, 0].add_patch(rect)\n",
    "    axes[i, 0].set_title('Original + Bounding Box')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Plot cropped\n",
    "    axes[i, 1].imshow(img_cropped, cmap='gray')\n",
    "    axes[i, 1].set_title('Cropped Region')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Plot resized\n",
    "    axes[i, 2].imshow(img_resized, cmap='gray')\n",
    "    axes[i, 2].set_title('Resized to 224√ó224')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Option 4: Context-Aware Crop + Resize', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('option4_smart_crop.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Smart cropping visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86297332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üîÑ Step 6: Context-Aware Augmentation\n",
    "# ============================================\n",
    "\n",
    "def apply_context_augmentation(image):\n",
    "    \"\"\"\n",
    "    Apply augmentations that preserve anatomical context\n",
    "    Suitable for already-cropped lung images\n",
    "    \"\"\"\n",
    "    # Small rotation to preserve anatomy\n",
    "    if random.random() > 0.5:\n",
    "        angle = random.uniform(-10, 10)\n",
    "        image = scipy_rotate(image, angle, reshape=False, mode='nearest')\n",
    "    \n",
    "    # Horizontal flip (chest X-rays can be left-right symmetric)\n",
    "    if random.random() > 0.5:\n",
    "        image = np.fliplr(image)\n",
    "    \n",
    "    # Brightness adjustment\n",
    "    if random.random() > 0.5:\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        image = np.clip(image * factor, 0, 1)\n",
    "    \n",
    "    # Contrast adjustment\n",
    "    if random.random() > 0.5:\n",
    "        mean = image.mean()\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        image = np.clip((image - mean) * factor + mean, 0, 1)\n",
    "    \n",
    "    # Gaussian noise\n",
    "    if random.random() > 0.6:\n",
    "        noise = np.random.normal(0, 0.02, image.shape)\n",
    "        image = np.clip(image + noise, 0, 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "print(\"‚úÖ Context-aware augmentation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e875396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üì¶ Step 7: Dataset Classes\n",
    "# ============================================\n",
    "\n",
    "class SegmentationGuidedDataset(Dataset):\n",
    "    \"\"\"SSL dataset with smart cropping + context-aware augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, df, img_size=224, padding=20, min_crop_ratio=0.5):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.img_size = img_size\n",
    "        self.padding = padding\n",
    "        self.min_crop_ratio = min_crop_ratio\n",
    "        \n",
    "        sample_paths = self.df['Image Path'].sample(min(200, len(self.df)), random_state=42).values\n",
    "        missing = [p for p in sample_paths if not os.path.exists(p)]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(f\"‚ùå Missing {len(missing)} images!\")\n",
    "        \n",
    "        print(f\"üì¶ SegmentationGuidedDataset: {len(self.df)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['Image Path']\n",
    "        \n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img = img.resize((self.img_size, self.img_size), Image.LANCZOS)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Segment and crop\n",
    "        lung_mask = segment_lungs(img)\n",
    "        img_cropped, mask_cropped = context_aware_crop(img, lung_mask, self.padding, self.min_crop_ratio)\n",
    "        \n",
    "        # Resize back\n",
    "        img_resized = cv2.resize(img_cropped, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Two augmented views\n",
    "        view1 = apply_context_augmentation(img_resized.copy())\n",
    "        view2 = apply_context_augmentation(img_resized.copy())\n",
    "        \n",
    "        view1 = torch.tensor(view1[None, ...], dtype=torch.float32)\n",
    "        view2 = torch.tensor(view2[None, ...], dtype=torch.float32)\n",
    "        \n",
    "        return view1, view2\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    \"\"\"Classification dataset with segmentation-guided preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, df, disease_categories, img_size=224, padding=20):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.disease_categories = disease_categories\n",
    "        self.img_size = img_size\n",
    "        self.padding = padding\n",
    "        \n",
    "        print(f\"üì¶ ClassificationDataset: {len(self.df)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img = Image.open(row['Image Path']).convert('L')\n",
    "        img = img.resize((self.img_size, self.img_size), Image.LANCZOS)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Segment and crop\n",
    "        lung_mask = segment_lungs(img)\n",
    "        img_cropped, _ = context_aware_crop(img, lung_mask, self.padding)\n",
    "        img_resized = cv2.resize(img_cropped, (self.img_size, self.img_size))\n",
    "        \n",
    "        img = torch.tensor(img_resized[None, ...], dtype=torch.float32)\n",
    "        labels = torch.tensor([row[d] for d in self.disease_categories], dtype=torch.float32)\n",
    "        \n",
    "        return img, labels\n",
    "\n",
    "print(\"‚úÖ Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üèóÔ∏è Step 8: Model Architecture\n",
    "# ============================================\n",
    "\n",
    "def conv_block(in_c, out_c, kernel=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel, stride, padding),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def residual_block(channels):\n",
    "    return nn.Sequential(\n",
    "        conv_block(channels, channels),\n",
    "        conv_block(channels, channels)\n",
    "    )\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, feat_dim=256):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(in_channels, 64), residual_block(64), nn.MaxPool2d(2),\n",
    "            conv_block(64, 128), residual_block(128), nn.MaxPool2d(2),\n",
    "            conv_block(128, 256), residual_block(256), residual_block(256), nn.MaxPool2d(2),\n",
    "            conv_block(256, 512), residual_block(512), residual_block(512), nn.MaxPool2d(2),\n",
    "            conv_block(512, 512), residual_block(512), nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, feat_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.fc(x.view(x.size(0), -1))\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, feat_dim=256, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_dim, feat_dim), nn.BatchNorm1d(feat_dim), nn.ReLU(),\n",
    "            nn.Linear(feat_dim, proj_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, feat_dim=256, img_size=224):\n",
    "        super().__init__()\n",
    "        self.init_size = img_size // 32\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256 * self.init_size * self.init_size), nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 4, 2, 1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        return self.decoder(x.view(z.size(0), 256, self.init_size, self.init_size))\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feat_dim=256, num_classes=14):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "encoder = Encoder(feat_dim=cfg.feat_dim).to(cfg.device)\n",
    "proj_head = ProjectionHead(cfg.feat_dim, cfg.proj_dim).to(cfg.device)\n",
    "decoder = Decoder(cfg.feat_dim, cfg.img_size).to(cfg.device)\n",
    "\n",
    "total_params = sum(p.numel() for m in [encoder, proj_head, decoder] for p in m.parameters())\n",
    "print(f\"‚úÖ Models initialized ({total_params:,} parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üî• Step 9: Loss Functions\n",
    "# ============================================\n",
    "\n",
    "def nt_xent_loss(z1, z2, temperature=0.1):\n",
    "    device = z1.device\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    batch_size = z1.shape[0]\n",
    "    representations = torch.cat([z1, z2], dim=0)\n",
    "    similarity = torch.matmul(representations, representations.T) / temperature\n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool, device=device)\n",
    "    similarity = similarity.masked_fill(mask, -float('inf'))\n",
    "    labels = torch.cat([torch.arange(batch_size) + batch_size,\n",
    "                        torch.arange(batch_size)]).to(device)\n",
    "    return F.cross_entropy(similarity, labels)\n",
    "\n",
    "print(\"‚úÖ Loss functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìä Step 10: Create Data Loaders\n",
    "# ============================================\n",
    "\n",
    "df_shuffled = df_labels.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_size = int(0.8 * len(df_shuffled))\n",
    "train_df = df_shuffled[:train_size]\n",
    "val_df = df_shuffled[train_size:]\n",
    "\n",
    "train_pretrain_ds = SegmentationGuidedDataset(train_df, img_size=cfg.img_size,\n",
    "                                              padding=cfg.crop_padding, \n",
    "                                              min_crop_ratio=cfg.min_crop_ratio)\n",
    "train_class_ds = ClassificationDataset(train_df, DISEASE_CATEGORIES, cfg.img_size, cfg.crop_padding)\n",
    "val_class_ds = ClassificationDataset(val_df, DISEASE_CATEGORIES, cfg.img_size, cfg.crop_padding)\n",
    "\n",
    "pretrain_loader = DataLoader(\n",
    "    train_pretrain_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=cfg.num_workers, pin_memory=True, drop_last=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_class_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=cfg.num_workers, pin_memory=True, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_class_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=cfg.num_workers, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üöÄ Step 11: SSL Pretraining\n",
    "# ============================================\n",
    "\n",
    "optimizer_ssl = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(proj_head.parameters()) + list(decoder.parameters()),\n",
    "    lr=cfg.lr_pretrain, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "ssl_history = {'loss': [], 'contrastive': [], 'reconstruction': []}\n",
    "START_EPOCH = 1\n",
    "\n",
    "if RESUME_SSL_PRETRAINING:\n",
    "    ckpt_file = find_latest_checkpoint(f'{OPTION_NAME}_ssl') if SSL_CHECKPOINT_FILE == \"latest\" else SSL_CHECKPOINT_FILE\n",
    "    if ckpt_file:\n",
    "        checkpoint = load_checkpoint(ckpt_file)\n",
    "        if checkpoint:\n",
    "            encoder.load_state_dict(checkpoint['encoder'])\n",
    "            proj_head.load_state_dict(checkpoint['proj_head'])\n",
    "            decoder.load_state_dict(checkpoint['decoder'])\n",
    "            if 'optimizer' in checkpoint: optimizer_ssl.load_state_dict(checkpoint['optimizer'])\n",
    "            ssl_history = checkpoint.get('ssl_history', ssl_history)\n",
    "            START_EPOCH = checkpoint['epoch'] + 1\n",
    "            print(f\"üîÑ Resuming from epoch {START_EPOCH}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No checkpoint found. Starting fresh.\")\n",
    "\n",
    "if START_EPOCH > cfg.pretrain_epochs:\n",
    "    print(f\"‚úÖ SSL Pretraining already complete\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Starting Option 4: Segmentation-Guided SSL Pretraining\")\n",
    "    print(f\"   Epochs: {START_EPOCH} ‚Üí {cfg.pretrain_epochs}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(START_EPOCH, cfg.pretrain_epochs + 1):\n",
    "        encoder.train()\n",
    "        proj_head.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        total_loss = total_cont = total_recon = 0\n",
    "        \n",
    "        pbar = tqdm(pretrain_loader, desc=f\"Epoch {epoch}/{cfg.pretrain_epochs}\")\n",
    "        for view1, view2 in pbar:\n",
    "            view1 = view1.to(cfg.device)\n",
    "            view2 = view2.to(cfg.device)\n",
    "            \n",
    "            optimizer_ssl.zero_grad()\n",
    "            \n",
    "            z1, z2 = encoder(view1), encoder(view2)\n",
    "            p1, p2 = proj_head(z1), proj_head(z2)\n",
    "            cont_loss = nt_xent_loss(p1, p2, cfg.temperature)\n",
    "            \n",
    "            rec1, rec2 = decoder(z1), decoder(z2)\n",
    "            recon_loss = (F.mse_loss(rec1, view1) + F.mse_loss(rec2, view2)) / 2\n",
    "            \n",
    "            loss = cont_loss + 0.5 * recon_loss\n",
    "            loss.backward()\n",
    "            optimizer_ssl.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_cont += cont_loss.item()\n",
    "            total_recon += recon_loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        n = len(pretrain_loader)\n",
    "        ssl_history['loss'].append(total_loss / n)\n",
    "        ssl_history['contrastive'].append(total_cont / n)\n",
    "        ssl_history['reconstruction'].append(total_recon / n)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss={total_loss/n:.4f}, Cont={total_cont/n:.4f}, Recon={total_recon/n:.4f}\")\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch, 'encoder': encoder.state_dict(),\n",
    "            'proj_head': proj_head.state_dict(), 'decoder': decoder.state_dict(),\n",
    "            'optimizer': optimizer_ssl.state_dict(), 'ssl_history': ssl_history,\n",
    "        }, f'{OPTION_NAME}_ssl_latest.pth')\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch, 'encoder': encoder.state_dict(),\n",
    "            'proj_head': proj_head.state_dict(), 'decoder': decoder.state_dict(),\n",
    "            'ssl_history': ssl_history,\n",
    "        }, f'{OPTION_NAME}_ssl_epoch{epoch}.pth')\n",
    "    \n",
    "    print(\"\\n‚úÖ Segmentation-Guided SSL Pretraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìà Step 12: Plot SSL Curves & Save Model\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(ssl_history['loss'], 'b-', linewidth=2)\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(ssl_history['contrastive'], 'r-', linewidth=2)\n",
    "axes[1].set_title('Contrastive Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(ssl_history['reconstruction'], 'g-', linewidth=2)\n",
    "axes[2].set_title('Reconstruction Loss')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Option 4: Segmentation-Guided SSL Training', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('option4_ssl_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "torch.save({\n",
    "    'encoder': encoder.state_dict(),\n",
    "    'proj_head': proj_head.state_dict(),\n",
    "    'decoder': decoder.state_dict(),\n",
    "}, 'option4_ssl_pretrained.pth')\n",
    "\n",
    "print(\"üíæ Pretrained model saved: option4_ssl_pretrained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üéØ Step 13-15: Fine-tuning\n",
    "# ============================================\n",
    "\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "encoder.eval()\n",
    "\n",
    "classifier = Classifier(cfg.feat_dim, len(DISEASE_CATEGORIES)).to(cfg.device)\n",
    "\n",
    "pos_counts = train_df[DISEASE_CATEGORIES].sum().values\n",
    "neg_counts = len(train_df) - pos_counts\n",
    "pos_weights = torch.tensor(neg_counts / (pos_counts + 1e-6), dtype=torch.float32).to(cfg.device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "optimizer_ft = torch.optim.Adam(classifier.parameters(), lr=cfg.lr_finetune, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'max', patience=5, factor=0.5)\n",
    "\n",
    "finetune_history = {'train_loss': [], 'train_auc': [], 'val_loss': [], 'val_auc': []}\n",
    "best_val_auc = 0\n",
    "FINETUNE_START_EPOCH = 1\n",
    "\n",
    "if RESUME_FINETUNING:\n",
    "    ckpt_file = find_latest_checkpoint(f'{OPTION_NAME}_finetune') if FINETUNE_CHECKPOINT_FILE == \"latest\" else FINETUNE_CHECKPOINT_FILE\n",
    "    if ckpt_file:\n",
    "        ft_ckpt = load_checkpoint(ckpt_file)\n",
    "        if ft_ckpt:\n",
    "            classifier.load_state_dict(ft_ckpt['classifier'])\n",
    "            if 'optimizer' in ft_ckpt: optimizer_ft.load_state_dict(ft_ckpt['optimizer'])\n",
    "            finetune_history = ft_ckpt.get('finetune_history', finetune_history)\n",
    "            best_val_auc = ft_ckpt.get('best_val_auc', 0)\n",
    "            FINETUNE_START_EPOCH = ft_ckpt['epoch'] + 1\n",
    "            print(f\"üîÑ Resuming fine-tuning from epoch {FINETUNE_START_EPOCH}\")\n",
    "\n",
    "if FINETUNE_START_EPOCH > cfg.finetune_epochs:\n",
    "    print(f\"‚úÖ Fine-tuning already complete\")\n",
    "else:\n",
    "    print(f\"\\nüéØ Starting Fine-tuning: Epochs {FINETUNE_START_EPOCH} ‚Üí {cfg.finetune_epochs}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for epoch in range(FINETUNE_START_EPOCH, cfg.finetune_epochs + 1):\n",
    "        classifier.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "        \n",
    "        for images, targets in tqdm(train_loader, desc=f\"Train {epoch}/{cfg.finetune_epochs}\"):\n",
    "            images, targets = images.to(cfg.device), targets.to(cfg.device)\n",
    "            optimizer_ft.zero_grad()\n",
    "            with torch.no_grad(): features = encoder(images)\n",
    "            logits = classifier(features)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer_ft.step()\n",
    "            train_loss += loss.item()\n",
    "            train_preds.append(torch.sigmoid(logits).detach().cpu())\n",
    "            train_targets.append(targets.cpu())\n",
    "        \n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images, targets = images.to(cfg.device), targets.to(cfg.device)\n",
    "                features = encoder(images)\n",
    "                logits = classifier(features)\n",
    "                val_loss += criterion(logits, targets).item()\n",
    "                val_preds.append(torch.sigmoid(logits).cpu())\n",
    "                val_targets.append(targets.cpu())\n",
    "        \n",
    "        train_preds, train_targets = torch.cat(train_preds).numpy(), torch.cat(train_targets).numpy()\n",
    "        val_preds, val_targets = torch.cat(val_preds).numpy(), torch.cat(val_targets).numpy()\n",
    "        \n",
    "        train_auc = np.mean([roc_auc_score(train_targets[:, i], train_preds[:, i]) \n",
    "                             for i in range(len(DISEASE_CATEGORIES)) if len(np.unique(train_targets[:, i])) > 1])\n",
    "        val_auc = np.mean([roc_auc_score(val_targets[:, i], val_preds[:, i]) \n",
    "                           for i in range(len(DISEASE_CATEGORIES)) if len(np.unique(val_targets[:, i])) > 1])\n",
    "        \n",
    "        finetune_history['train_loss'].append(train_loss / len(train_loader))\n",
    "        finetune_history['train_auc'].append(train_auc)\n",
    "        finetune_history['val_loss'].append(val_loss / len(val_loader))\n",
    "        finetune_history['val_auc'].append(val_auc)\n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train AUC={train_auc:.4f}, Val AUC={val_auc:.4f}\")\n",
    "        \n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            save_checkpoint({'encoder': encoder.state_dict(), 'classifier': classifier.state_dict(),\n",
    "                            'val_auc': val_auc, 'epoch': epoch}, f'{OPTION_NAME}_best_model.pth')\n",
    "            print(f\"  ‚úÖ Best model saved! Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == cfg.finetune_epochs:\n",
    "            save_checkpoint({'epoch': epoch, 'classifier': classifier.state_dict(),\n",
    "                            'optimizer': optimizer_ft.state_dict(), 'finetune_history': finetune_history,\n",
    "                            'best_val_auc': best_val_auc}, f'{OPTION_NAME}_finetune_latest.pth')\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f63080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìä Step 16: Final Evaluation & Results\n",
    "# ============================================\n",
    "\n",
    "checkpoint = torch.load('option4_best_model.pth')\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "classifier.load_state_dict(checkpoint['classifier'])\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(cfg.device)\n",
    "        features = encoder(images)\n",
    "        logits = classifier(features)\n",
    "        all_preds.append(torch.sigmoid(logits).cpu())\n",
    "        all_targets.append(targets)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "print(\"\\nüìä Per-Disease AUC Scores:\")\n",
    "print(\"=\" * 40)\n",
    "auc_scores = []\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(all_targets[:, i])) > 1:\n",
    "        auc = roc_auc_score(all_targets[:, i], all_preds[:, i])\n",
    "        auc_scores.append((disease, auc))\n",
    "        print(f\"{disease:20s}: {auc:.4f}\")\n",
    "\n",
    "mean_auc = np.mean([a for _, a in auc_scores])\n",
    "print(f\"\\n{'Mean AUC':20s}: {mean_auc:.4f}\")\n",
    "\n",
    "auc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "diseases, aucs = zip(*auc_scores)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['green' if a >= 0.7 else 'orange' if a >= 0.6 else 'red' for a in aucs]\n",
    "plt.barh(diseases, aucs, color=colors, alpha=0.8)\n",
    "plt.axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "plt.axvline(mean_auc, color='blue', linestyle='--', alpha=0.7, label=f'Mean: {mean_auc:.3f}')\n",
    "plt.xlabel('AUC Score')\n",
    "plt.title('Option 4: Per-Disease AUC Performance (RECOMMENDED)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('option4_auc_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚≠ê OPTION 4: SEGMENTATION-GUIDED SSL (RECOMMENDED) SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Method: Context-Aware Crop + Context-Preserving Augmentation\")\n",
    "print(f\"\\nKey Advantages:\")\n",
    "print(f\"  ‚úÖ Eliminates non-anatomical background noise\")\n",
    "print(f\"  ‚úÖ Focuses SSL on lung tissue patterns\")\n",
    "print(f\"  ‚úÖ Reduces imaging artifact bias\")\n",
    "print(f\"  ‚úÖ Better convergence and faster training\")\n",
    "print(f\"  ‚úÖ Preserves anatomical context\")\n",
    "print(f\"\\nüèÜ Final Mean AUC: {mean_auc:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
