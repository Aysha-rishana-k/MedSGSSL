{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2647816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported\n",
      "ðŸ–¥ï¸ Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ“¦ Step 1: Import Libraries\n",
    "# ============================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, sys, copy, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "try:\n",
    "    import kagglehub\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "os.environ['OPENCV_LOG_LEVEL'] = 'SILENT'  # Suppress libpng ICC warnings\n",
    "\n",
    "print(\"âœ… All libraries imported\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸ–¥ï¸ Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1362701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 1.0.0)\n",
      "ðŸ“‚ Base path: /root/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3\n",
      "âœ… Loaded 112,120 images with 14 disease labels\n",
      "ðŸ“‚ Image directories: 12\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ“ Step 2: Download and Load Dataset\n",
    "# ============================================\n",
    "\n",
    "IN_KAGGLE = os.path.exists('/kaggle/input')\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    BASE_PATH = Path('/kaggle/input/nih-chest-xrays')\n",
    "elif IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = Path('/content/nih-chest-xrays')\n",
    "else:\n",
    "    path = kagglehub.dataset_download(\"nih-chest-xrays/data\")\n",
    "    BASE_PATH = Path(path)\n",
    "\n",
    "print(f\"ðŸ“‚ Base path: {BASE_PATH}\")\n",
    "\n",
    "# Load metadata\n",
    "csv_path = BASE_PATH / 'Data_Entry_2017.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "DISEASE_CATEGORIES = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass',\n",
    "    'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia'\n",
    "]\n",
    "\n",
    "for disease in DISEASE_CATEGORIES:\n",
    "    df[disease] = df['Finding Labels'].apply(lambda x: 1 if disease in x else 0)\n",
    "\n",
    "# Find image paths\n",
    "image_dirs = list(BASE_PATH.glob('images_*/images'))\n",
    "if not image_dirs:\n",
    "    image_dirs = [BASE_PATH / 'images']\n",
    "\n",
    "image_map = {}\n",
    "for d in image_dirs:\n",
    "    for f in d.iterdir():\n",
    "        if f.suffix == '.png':\n",
    "            image_map[f.name] = str(f)\n",
    "\n",
    "df['Image Path'] = df['Image Index'].map(image_map)\n",
    "df = df.dropna(subset=['Image Path']).reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Loaded {len(df):,} images with {len(DISEASE_CATEGORIES)} disease labels\")\n",
    "print(f\"ðŸ“‚ Image directories: {len(image_dirs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf71c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration ready\n",
      "   ViT-Small: embed_dim=384, heads=6, depth=12\n",
      "   Pretrained: True, Freeze epochs: 5\n",
      "   Patches: 196, mask_ratio=0.5\n",
      "   Checkpoint dir: ./checkpoints_option7_anatomy_masked_attention_ssl\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# âš™ï¸ Step 3: Configuration\n",
    "# ============================================\n",
    "\n",
    "OPTION_NAME = \"option7_anatomy_masked_attention_ssl\"\n",
    "\n",
    "class Config:\n",
    "    # ViT-Small settings (matches timm vit_small_patch16_224)\n",
    "    img_size = 224\n",
    "    patch_size = 16\n",
    "    num_patches = (224 // 16) ** 2  # 196\n",
    "    embed_dim = 384          # ViT-Small (was 192 for Tiny)\n",
    "    num_heads = 6            # ViT-Small (was 3 for Tiny)\n",
    "    depth = 12\n",
    "    mlp_ratio = 4.0\n",
    "\n",
    "    # Pretrained settings\n",
    "    use_pretrained = True    # Load ImageNet pretrained weights\n",
    "    freeze_epochs = 5        # Freeze encoder for first N SSL epochs\n",
    "\n",
    "    # SSL\n",
    "    feat_dim = 384           # Match embed_dim\n",
    "    proj_dim = 128\n",
    "    mask_ratio = 0.5\n",
    "    momentum = 0.996\n",
    "    lambda_attn = 0.05\n",
    "    lambda_contrastive = 500   # contrastive loss weight\n",
    "\n",
    "    # Training\n",
    "    batch_size = 64          # Reduced due to larger model\n",
    "    pretrain_epochs = 50\n",
    "    finetune_epochs = 50\n",
    "    lr_pretrain = 1e-4\n",
    "    lr_finetune = 1e-4\n",
    "\n",
    "    device = device\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    CHECKPOINT_DIR = f'/kaggle/working/checkpoints_{OPTION_NAME}'\n",
    "elif IN_COLAB:\n",
    "    CHECKPOINT_DIR = f'/content/drive/MyDrive/checkpoints_{OPTION_NAME}'\n",
    "else:\n",
    "    CHECKPOINT_DIR = f'./checkpoints_{OPTION_NAME}'\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Configuration ready\")\n",
    "print(f\"   ViT-Small: embed_dim={cfg.embed_dim}, heads={cfg.num_heads}, depth={cfg.depth}\")\n",
    "print(f\"   Pretrained: {cfg.use_pretrained}, Freeze epochs: {cfg.freeze_epochs}\")\n",
    "print(f\"   Patches: {cfg.num_patches}, mask_ratio={cfg.mask_ratio}\")\n",
    "print(f\"   Checkpoint dir: {CHECKPOINT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977d6e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Checkpoint utilities ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ’¾ Step 3.5: Checkpoint & Resume Configuration\n",
    "# ============================================\n",
    "\n",
    "RESUME_SSL_PRETRAINING = True\n",
    "RESUME_FINETUNING = True\n",
    "SSL_CHECKPOINT_FILE = \"latest\"\n",
    "FINETUNE_CHECKPOINT_FILE = \"latest\"\n",
    "SAVE_EVERY_SSL = 5\n",
    "SAVE_EVERY_FT = 5\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    path = os.path.join(CHECKPOINT_DIR, filename)\n",
    "    torch.save(state, path)\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    path = os.path.join(CHECKPOINT_DIR, filename)\n",
    "    if os.path.exists(path):\n",
    "        return torch.load(path, map_location=cfg.device, weights_only=False)\n",
    "    return None\n",
    "\n",
    "print(\"âœ… Checkpoint utilities ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad56aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch mask directory: ./lung_masks/patch_masks\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Step 4: Load Pre-Computed Lung Masks\n",
    "# ============================================\n",
    "# Masks pre-computed by precompute_lung_masks.ipynb\n",
    "# Replaces segment_lungs() and lung_mask_to_patch_mask()\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    PATCH_MASK_DIR = '/kaggle/working/lung_masks/patch_masks'\n",
    "elif IN_COLAB:\n",
    "    PATCH_MASK_DIR = '/content/drive/MyDrive/lung_masks/patch_masks'\n",
    "else:\n",
    "    PATCH_MASK_DIR = './lung_masks/patch_masks'\n",
    "\n",
    "def load_precomputed_patch_masks(dataframe, mask_dir=PATCH_MASK_DIR):\n",
    "    \"\"\"Bulk-load all patch masks for a dataframe into a list.\"\"\"\n",
    "    masks = []\n",
    "    missing = 0\n",
    "    for img_name in tqdm(dataframe[\"Image Index\"], desc=\"Loading patch masks\"):\n",
    "        mask_name = img_name.replace(\".png\", \"\")\n",
    "        mask_path = os.path.join(mask_dir, f\"{mask_name}.npy\")\n",
    "        if os.path.exists(mask_path):\n",
    "            masks.append(np.load(mask_path))\n",
    "        else:\n",
    "            # Fallback: all-ones mask (treat entire image as lung)\n",
    "            masks.append(np.ones(196, dtype=np.float32))\n",
    "            missing += 1\n",
    "    if missing > 0:\n",
    "        print(f\"Warning: {missing} masks not found, using all-ones fallback\")\n",
    "    print(f\"Loaded {len(masks)} patch masks from {mask_dir}\")\n",
    "    return masks\n",
    "\n",
    "print(f\"Patch mask directory: {PATCH_MASK_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbeb6df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Augmentation pipeline ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ”„ Step 5: Data Augmentation\n",
    "# ============================================\n",
    "\n",
    "class ChestXrayAugment:\n",
    "    \"\"\"Augmentations for chest X-ray SSL (cv2-based for speed).\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224):\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"img: numpy array (H, W) float32 [0,1]\"\"\"\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.numpy()\n",
    "        if img.ndim == 3 and img.shape[0] == 1:\n",
    "            img = img[0]\n",
    "\n",
    "        # Random horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            img = np.fliplr(img).copy()\n",
    "\n",
    "        # Random rotation (cv2 â€” much faster than scipy)\n",
    "        if random.random() < 0.7:\n",
    "            angle = random.uniform(-10, 10)\n",
    "            h, w = img.shape[:2]\n",
    "            M = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1.0)\n",
    "            img = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "        # Brightness\n",
    "        if random.random() < 0.8:\n",
    "            factor = 1 + random.uniform(-0.15, 0.15)\n",
    "            img = np.clip(img * factor, 0, 1)\n",
    "\n",
    "        # Contrast\n",
    "        if random.random() < 0.8:\n",
    "            mean = img.mean()\n",
    "            factor = random.uniform(0.85, 1.15)\n",
    "            img = np.clip((img - mean) * factor + mean, 0, 1)\n",
    "\n",
    "        # Gaussian noise\n",
    "        if random.random() < 0.3:\n",
    "            noise = np.random.randn(*img.shape).astype(np.float32) * 0.03\n",
    "            img = np.clip(img + noise, 0, 1)\n",
    "\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "augment = ChestXrayAugment(cfg.img_size)\n",
    "print(\"âœ… Augmentation pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7080931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ“¦ Step 6: Dataset Classes\n",
    "# ============================================\n",
    "\n",
    "class Option7SSLDataset(Dataset):\n",
    "    \"\"\"SSL dataset with PRE-COMPUTED patch masks (no on-the-fly segmentation).\"\"\"\n",
    "\n",
    "    def __init__(self, df, precomputed_patch_masks, img_size=224, augment_fn=None):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.patch_masks = precomputed_patch_masks  # list of (N,) np arrays\n",
    "        self.img_size = img_size\n",
    "        self.augment_fn = augment_fn\n",
    "        self.paths = df['Image Path'].tolist()\n",
    "        print(f\"ðŸ“¦ Option7SSLDataset: {len(self.df)} samples (masks pre-computed)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # âš¡ cv2 is ~30-50% faster than PIL for loading + resize\n",
    "        img_np = cv2.imread(self.paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        img_np = cv2.resize(img_np, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "        img_np = img_np.astype(np.float32) / 255.0\n",
    "\n",
    "        # âš¡ Use pre-computed patch mask (no segmentation here!)\n",
    "        patch_mask = self.patch_masks[idx]\n",
    "\n",
    "        # Augment image\n",
    "        if self.augment_fn:\n",
    "            img_np = self.augment_fn(img_np)\n",
    "\n",
    "        # To tensor: (1, H, W)\n",
    "        img_tensor = torch.from_numpy(img_np).unsqueeze(0)\n",
    "        patch_mask_tensor = torch.from_numpy(patch_mask)\n",
    "\n",
    "        return img_tensor, patch_mask_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    \"\"\"Multi-label classification dataset (cv2-based loading).\"\"\"\n",
    "\n",
    "    def __init__(self, df, disease_categories, img_size=224, is_training=False):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.disease_categories = disease_categories\n",
    "        self.img_size = img_size\n",
    "        self.is_training = is_training\n",
    "        # Pre-extract labels to avoid repeated iloc lookups\n",
    "        self.labels = torch.tensor(\n",
    "            df[disease_categories].values.astype(np.float32)\n",
    "        )\n",
    "        self.paths = df['Image Path'].tolist()\n",
    "        print(f\"ðŸ“¦ ClassificationDataset: {len(self.df)} samples (training={is_training})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # âš¡ cv2 loading\n",
    "        img_np = cv2.imread(self.paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        img_np = cv2.resize(img_np, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "        img_np = img_np.astype(np.float32) / 255.0\n",
    "\n",
    "        if self.is_training:\n",
    "            if random.random() < 0.5:\n",
    "                img_np = np.fliplr(img_np).copy()\n",
    "            if random.random() < 0.5:\n",
    "                angle = random.uniform(-10, 10)\n",
    "                h, w = img_np.shape[:2]\n",
    "                M = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1.0)\n",
    "                img_np = cv2.warpAffine(img_np, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "            if random.random() < 0.5:\n",
    "                factor = 1 + random.uniform(-0.15, 0.15)\n",
    "                img_np = np.clip(img_np * factor, 0, 1)\n",
    "\n",
    "        img = torch.from_numpy(img_np).unsqueeze(0)  # (1, H, W)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "print(\"âœ… Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d29daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Models initialized (pretrained=True)\n",
      "   Online ViT-Small : 21,469,056 params\n",
      "   Recon head       : 590,976 params\n",
      "   Freeze epochs    : 5\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ—ï¸ Step 7: Pretrained ViT-Small Encoder & Momentum Teacher\n",
    "# ============================================\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"ViT-Small encoder built on timm, exposing attention maps.\n",
    "    \n",
    "    Uses pretrained ImageNet weights, adapted for 1-channel grayscale input.\n",
    "    Attention maps are captured via forward hooks for the attention alignment loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=1,\n",
    "                 embed_dim=384, depth=12, num_heads=6, mlp_ratio=4.0,\n",
    "                 use_pretrained=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Load pretrained ViT-Small from timm\n",
    "        self.vit = timm.create_model(\n",
    "            'vit_small_patch16_224',\n",
    "            pretrained=use_pretrained,\n",
    "            in_chans=3,  # Load with 3 channels first, then adapt\n",
    "            num_classes=0,  # Remove classification head\n",
    "            img_size=img_size,\n",
    "        )\n",
    "        \n",
    "        # Adapt patch embedding for 1-channel grayscale input\n",
    "        if in_chans == 1:\n",
    "            old_proj = self.vit.patch_embed.proj\n",
    "            # Average the 3 RGB channels into 1\n",
    "            new_proj = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "            with torch.no_grad():\n",
    "                new_proj.weight.copy_(old_proj.weight.mean(dim=1, keepdim=True))\n",
    "                new_proj.bias.copy_(old_proj.bias)\n",
    "            self.vit.patch_embed.proj = new_proj\n",
    "        \n",
    "        # Storage for attention weights (populated by hooks)\n",
    "        self._attn_weights = [None] * len(self.vit.blocks)\n",
    "        self._hooks = []\n",
    "        self._register_attn_hooks()\n",
    "    \n",
    "    def _register_attn_hooks(self):\n",
    "        \"\"\"Register forward hooks to capture attention weights from each block.\"\"\"\n",
    "        for i, block in enumerate(self.vit.blocks):\n",
    "            def hook_fn(module, input, output, idx=i):\n",
    "                # timm's Attention module computes attn internally\n",
    "                # We need to recompute it from q, k for extraction\n",
    "                pass  # We'll use a different approach below\n",
    "            # Instead, we override forward to capture attention\n",
    "        self._patch_attention_forward()\n",
    "    \n",
    "    def _patch_attention_forward(self):\n",
    "        \"\"\"Patch each block's attention to store attention weights.\"\"\"\n",
    "        for i, block in enumerate(self.vit.blocks):\n",
    "            original_attn_forward = block.attn.forward\n",
    "            attn_weights_store = self._attn_weights\n",
    "            block_idx = i\n",
    "            \n",
    "            def make_new_forward(orig_fn, store, idx):\n",
    "                def new_forward(x, **kwargs):\n",
    "                    B, N, C = x.shape\n",
    "                    qkv = block.attn.qkv(x).reshape(B, N, 3, block.attn.num_heads, C // block.attn.num_heads)\n",
    "                    qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "                    q, k, v = qkv.unbind(0)\n",
    "                    \n",
    "                    scale = (C // block.attn.num_heads) ** -0.5\n",
    "                    attn = (q @ k.transpose(-2, -1)) * scale\n",
    "                    attn = attn.softmax(dim=-1)\n",
    "                    store[idx] = attn  # Save attention weights\n",
    "                    \n",
    "                    attn_dropped = block.attn.attn_drop(attn)\n",
    "                    x_out = (attn_dropped @ v).transpose(1, 2).reshape(B, N, C)\n",
    "                    x_out = block.attn.proj(x_out)\n",
    "                    x_out = block.attn.proj_drop(x_out)\n",
    "                    return x_out\n",
    "                return new_forward\n",
    "            \n",
    "            block.attn.forward = make_new_forward(original_attn_forward, attn_weights_store, block_idx)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass returning all tokens (CLS + patches).\"\"\"\n",
    "        # Use timm's forward_features but return all tokens\n",
    "        x = self.vit.patch_embed(x)\n",
    "        cls_token = self.vit.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.vit.pos_embed\n",
    "        x = self.vit.pos_drop(x)\n",
    "        \n",
    "        for blk in self.vit.blocks:\n",
    "            x = blk(x)\n",
    "        \n",
    "        x = self.vit.norm(x)\n",
    "        return x  # (B, 1+N, D)\n",
    "    \n",
    "    def get_attention_maps(self):\n",
    "        \"\"\"Return attention weight tensors from each block.\"\"\"\n",
    "        return list(self._attn_weights)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"Projection head for contrastive learning on CLS tokens.\"\"\"\n",
    "    def __init__(self, embed_dim=384, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, proj_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ReconstructionHead(nn.Module):\n",
    "    \"\"\"MLP head to reconstruct masked patch features.\"\"\"\n",
    "    def __init__(self, embed_dim=384, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"Multi-label classifier on CLS token.\"\"\"\n",
    "    def __init__(self, feat_dim=384, num_classes=14):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# â”€â”€ Momentum Teacher Utilities â”€â”€\n",
    "@torch.no_grad()\n",
    "def ema_update(online, teacher, momentum):\n",
    "    \"\"\"Exponential moving average update for teacher.\"\"\"\n",
    "    for p_o, p_t in zip(online.parameters(), teacher.parameters()):\n",
    "        p_t.data.mul_(momentum).add_(p_o.data, alpha=1.0 - momentum)\n",
    "\n",
    "\n",
    "def freeze_encoder(encoder):\n",
    "    \"\"\"Freeze all encoder parameters (for initial SSL warm-up).\"\"\"\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Keep the patch embedding trainable (adapting from RGBâ†’grayscale)\n",
    "    for param in encoder.vit.patch_embed.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"ðŸ”’ Encoder frozen (except patch embedding)\")\n",
    "\n",
    "\n",
    "def unfreeze_encoder(encoder):\n",
    "    \"\"\"Unfreeze all encoder parameters.\"\"\"\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"ðŸ”“ Encoder unfrozen â€” all parameters trainable\")\n",
    "\n",
    "\n",
    "# â”€â”€ Initialize models â”€â”€\n",
    "online_encoder = ViTEncoder(\n",
    "    img_size=cfg.img_size, patch_size=cfg.patch_size, in_chans=1,\n",
    "    embed_dim=cfg.embed_dim, depth=cfg.depth,\n",
    "    num_heads=cfg.num_heads, mlp_ratio=cfg.mlp_ratio,\n",
    "    use_pretrained=cfg.use_pretrained\n",
    ").to(cfg.device)\n",
    "\n",
    "teacher_encoder = copy.deepcopy(online_encoder)\n",
    "for p in teacher_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "recon_head = ReconstructionHead(cfg.embed_dim, cfg.embed_dim * 2).to(cfg.device)\n",
    "proj_head = ProjectionHead(cfg.embed_dim, cfg.proj_dim).to(cfg.device)\n",
    "\n",
    "total_params = sum(p.numel() for p in online_encoder.parameters())\n",
    "print(f\"âœ… Models initialized (pretrained={cfg.use_pretrained})\")\n",
    "print(f\"   Online ViT-Small : {total_params:,} params\")\n",
    "print(f\"   Recon head       : {sum(p.numel() for p in recon_head.parameters()):,} params\")\n",
    "print(f\"   Freeze epochs    : {cfg.freeze_epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb75e504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loss functions: masked_reconstruction_loss, attention_alignment_loss\n",
      "âœ… Loss functions: masked_reconstruction_loss, attention_alignment_loss, nt_xent_loss\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ“‰ Step 8: Loss Functions\n",
    "# ============================================\n",
    "\n",
    "def masked_reconstruction_loss(recon, target, mask):\n",
    "    \"\"\"\n",
    "    MSE reconstruction loss on masked patch positions only.\n",
    "    \n",
    "    Args:\n",
    "        recon:  (B, N, D) reconstructed patch embeddings\n",
    "        target: (B, N, D) teacher patch embeddings (reconstruction target)\n",
    "        mask:   (B, N)   binary mask (1 = masked/reconstruct, 0 = visible)\n",
    "    Returns:\n",
    "        Scalar loss: mean squared error averaged over masked positions\n",
    "    \"\"\"\n",
    "    mask_exp = mask.unsqueeze(-1)  # (B, N, 1)\n",
    "    diff = (recon - target) ** 2   # (B, N, D)\n",
    "    return (diff * mask_exp).sum() / mask_exp.sum().clamp(min=1.0)\n",
    "\n",
    "\n",
    "def attention_alignment_loss(attn_maps, patch_masks):\n",
    "    \"\"\"\n",
    "    Soft attention guidance: encourage CLS token to attend to lung patches.\n",
    "    Uses cross-entropy between CLS-to-patch attention and a soft lung prior.\n",
    "    \n",
    "    Args:\n",
    "        attn_maps:   list of (B, H, T, T) attention tensors (T = 1+N, includes CLS)\n",
    "        patch_masks: (B, N) lung patch masks (1 = lung, 0 = background)\n",
    "    Returns:\n",
    "        Scalar loss averaged across layers and heads\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    n_layers = 0\n",
    "    for attn in attn_maps:\n",
    "        if attn is None:\n",
    "            continue\n",
    "        # CLS-to-patch attention: row 0, cols 1: â†’ (B, H, N)\n",
    "        cls_attn = attn[:, :, 0, 1:]  # (B, H, N)\n",
    "        N = cls_attn.shape[-1]\n",
    "        \n",
    "        # Soft target: lung=1.0, non-lung=0.1, normalized to probability dist\n",
    "        lung = patch_masks[:, :N].float()          # (B, N)\n",
    "        prior = lung * 0.9 + 0.1                   # lung=1.0, bg=0.1\n",
    "        prior = prior / prior.sum(dim=-1, keepdim=True)  # (B, N)\n",
    "        prior = prior.unsqueeze(1)                 # (B, 1, N)\n",
    "        \n",
    "        # Cross-entropy: -sum(prior * log(attn))\n",
    "        log_attn = torch.log(cls_attn.clamp(min=1e-8))\n",
    "        ce = -(prior * log_attn).sum(dim=-1)       # (B, H)\n",
    "        total_loss += ce.mean()\n",
    "        n_layers += 1\n",
    "    return total_loss / max(n_layers, 1)\n",
    "\n",
    "print(\"\\u2705 Loss functions: masked_reconstruction_loss, attention_alignment_loss\")\n",
    "\n",
    "def nt_xent_loss(z1, z2, temperature=0.1):\n",
    "    \"\"\"\n",
    "    NT-Xent (Normalized Temperature-scaled Cross-Entropy) contrastive loss.\n",
    "    Pulls together CLS tokens from two masked views of the same image.\n",
    "    \"\"\"\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    batch_size = z1.shape[0]\n",
    "    representations = torch.cat([z1, z2], dim=0)\n",
    "    similarity = torch.matmul(representations, representations.T) / temperature\n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z1.device)\n",
    "    similarity = similarity.masked_fill(mask, -float(\"inf\"))\n",
    "    labels = torch.cat([torch.arange(batch_size) + batch_size,\n",
    "                        torch.arange(batch_size)]).to(z1.device)\n",
    "    return F.cross_entropy(similarity, labels)\n",
    "\n",
    "print(\"âœ… Loss functions: masked_reconstruction_loss, attention_alignment_loss, nt_xent_loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c9510ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ”€ PATIENT-LEVEL SPLITTING\n",
      "============================================================\n",
      "Total unique patients: 30,805\n",
      "Train: 103,634  Val: 6,187  Test: 2,299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading patch masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103634/103634 [00:19<00:00, 5313.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 103634 patch masks from ./lung_masks/patch_masks\n",
      "ðŸ“¦ Option7SSLDataset: 103634 samples (masks pre-computed)\n",
      "ðŸ“¦ ClassificationDataset: 103634 samples (training=True)\n",
      "ðŸ“¦ ClassificationDataset: 6187 samples (training=False)\n",
      "ðŸ“¦ ClassificationDataset: 2299 samples (training=False)\n",
      "âœ… DataLoaders ready (optimized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ“Š Step 9: Create Data Loaders (Patient-Level Split)\n",
    "# ============================================\n",
    "\n",
    "NUM_WORKERS = 8  # âš¡ Increase for faster data loading (was 2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ”€ PATIENT-LEVEL SPLITTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "unique_patients = df['Patient ID'].unique()\n",
    "print(f\"Total unique patients: {len(unique_patients):,}\")\n",
    "\n",
    "train_val_patients, test_patients = train_test_split(\n",
    "    unique_patients, test_size=0.02, random_state=42\n",
    ")\n",
    "train_patients, val_patients = train_test_split(\n",
    "    train_val_patients, test_size=0.054, random_state=42\n",
    ")\n",
    "\n",
    "train_df = df[df['Patient ID'].isin(train_patients)].reset_index(drop=True)\n",
    "val_df = df[df['Patient ID'].isin(val_patients)].reset_index(drop=True)\n",
    "test_df = df[df['Patient ID'].isin(test_patients)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df):,}  Val: {len(val_df):,}  Test: {len(test_df):,}\")\n",
    "\n",
    "# â”€â”€ Load pre-computed lung patch masks from disk (fast!) â”€â”€\n",
    "precomputed_masks = load_precomputed_patch_masks(train_df)\n",
    "\n",
    "# SSL datasets (with pre-computed masks)\n",
    "ssl_dataset = Option7SSLDataset(train_df, precomputed_masks, cfg.img_size, augment)\n",
    "ssl_loader = DataLoader(ssl_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True, drop_last=True,\n",
    "                        persistent_workers=True, prefetch_factor=3)\n",
    "\n",
    "# Classification datasets\n",
    "train_cls_dataset = ClassificationDataset(train_df, DISEASE_CATEGORIES, cfg.img_size, is_training=True)\n",
    "val_cls_dataset = ClassificationDataset(val_df, DISEASE_CATEGORIES, cfg.img_size, is_training=False)\n",
    "test_cls_dataset = ClassificationDataset(test_df, DISEASE_CATEGORIES, cfg.img_size, is_training=False)\n",
    "\n",
    "train_loader = DataLoader(train_cls_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                          persistent_workers=True, prefetch_factor=3)\n",
    "val_loader = DataLoader(val_cls_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                        persistent_workers=True, prefetch_factor=3)\n",
    "test_loader = DataLoader(test_cls_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
    "                         num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                         persistent_workers=True, prefetch_factor=3)\n",
    "\n",
    "print(\"âœ… DataLoaders ready (optimized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”’ Encoder frozen (except patch embedding)\n",
      "ðŸ†• Starting SSL from scratch\n",
      "\n",
      "============================================================\n",
      "ðŸš€ OPTION 7: Anatomy-Constrained Masked Attention SSL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SSL Epoch 1/50:   0%|          | 0/1619 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸš€ Step 10: Anatomy-Constrained Masked Attention SSL Pretraining\n",
    "# ============================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "optimizer_ssl = torch.optim.AdamW(\n",
    "    list(online_encoder.parameters()) + list(recon_head.parameters()) + list(proj_head.parameters()),\n",
    "    lr=cfg.lr_pretrain, weight_decay=0.05\n",
    ")\n",
    "scheduler_ssl = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_ssl, T_max=cfg.pretrain_epochs, eta_min=1e-6\n",
    ")\n",
    "\n",
    "ssl_history = {'loss': [], 'recon_loss': [], 'attn_loss': [], 'cont_loss': []}\n",
    "start_epoch = 1\n",
    "\n",
    "# Apply initial freeze if configured\n",
    "if cfg.freeze_epochs > 0 and start_epoch <= cfg.freeze_epochs:\n",
    "    freeze_encoder(online_encoder)\n",
    "\n",
    "# Resume logic\n",
    "if RESUME_SSL_PRETRAINING:\n",
    "    ckpt_name = f'{OPTION_NAME}_ssl_latest.pth'\n",
    "    ckpt = load_checkpoint(ckpt_name)\n",
    "    if ckpt is not None:\n",
    "        online_encoder.load_state_dict(ckpt['online_encoder'])\n",
    "        teacher_encoder.load_state_dict(ckpt['teacher_encoder'])\n",
    "        recon_head.load_state_dict(ckpt['recon_head'])\n",
    "        if 'proj_head' in ckpt: proj_head.load_state_dict(ckpt['proj_head'])\n",
    "        optimizer_ssl.load_state_dict(ckpt['optimizer'])\n",
    "        ssl_history = ckpt.get('ssl_history', ssl_history)\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        print(f\"âœ… Resumed SSL from epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(\"ðŸ†• Starting SSL from scratch\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸš€ OPTION 7: Anatomy-Constrained Masked Attention SSL\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for epoch in range(start_epoch, cfg.pretrain_epochs + 1):\n",
    "    # Unfreeze encoder after freeze_epochs\n",
    "    if epoch == cfg.freeze_epochs + 1:\n",
    "        unfreeze_encoder(online_encoder)\n",
    "        # Re-create optimizer to include newly unfrozen params\n",
    "        optimizer_ssl = torch.optim.AdamW(\n",
    "            list(online_encoder.parameters()) + list(recon_head.parameters()),\n",
    "            lr=cfg.lr_pretrain, weight_decay=0.05\n",
    "        )\n",
    "        scheduler_ssl = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer_ssl, T_max=cfg.pretrain_epochs - cfg.freeze_epochs, eta_min=1e-6\n",
    "        )\n",
    "        print(f'ðŸ”„ Optimizer re-created for unfrozen encoder')\n",
    "\n",
    "    online_encoder.train()\n",
    "    recon_head.train()\n",
    "    proj_head.train()\n",
    "    teacher_encoder.eval()\n",
    "\n",
    "    epoch_loss, epoch_recon, epoch_attn, epoch_cont = 0.0, 0.0, 0.0, 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    loader = tqdm(ssl_loader, desc=f\"SSL Epoch {epoch}/{cfg.pretrain_epochs}\") if not IN_KAGGLE else ssl_loader\n",
    "\n",
    "    for images, patch_masks in loader:\n",
    "        images = images.to(cfg.device)          # (B, 1, H, W)\n",
    "        patch_masks = patch_masks.to(cfg.device) # (B, N)\n",
    "\n",
    "        # â”€â”€ 1. Teacher forward (no mask, no grad) â”€â”€\n",
    "        with torch.no_grad():\n",
    "            teacher_tokens = teacher_encoder(images)          # (B, 1+N, D)\n",
    "            teacher_patch = teacher_tokens[:, 1:, :]          # (B, N, D)\n",
    "\n",
    "        # â”€â”€ 2. Online encoder forward â”€â”€\n",
    "        online_tokens = online_encoder(images)                # (B, 1+N, D)\n",
    "        online_patch = online_tokens[:, 1:, :]                # (B, N, D)\n",
    "\n",
    "        # â”€â”€ 3. Lung-aware masking (vectorized) â”€â”€\n",
    "        B, N, D = online_patch.shape\n",
    "        lung_float = (patch_masks > 0.5).float()  # (B, N)\n",
    "        n_lung = lung_float.sum(dim=1, keepdim=True).clamp(min=1)  # (B, 1)\n",
    "        n_mask = (n_lung * cfg.mask_ratio).long().clamp(min=1)  # (B, 1)\n",
    "        # Assign random priority to lung patches, -inf to non-lung\n",
    "        noise = torch.rand(B, N, device=cfg.device)\n",
    "        noise = noise.masked_fill(lung_float < 0.5, -1.0)  # non-lung â†’ never selected\n",
    "        # Select top n_mask patches per sample\n",
    "        _, sorted_idx = noise.sort(dim=1, descending=True)\n",
    "        rank = torch.zeros_like(noise)\n",
    "        rank.scatter_(1, sorted_idx, torch.arange(N, device=cfg.device).float().unsqueeze(0).expand(B, -1))\n",
    "        mask_indices = (rank < n_mask).float()  # (B, N)\n",
    "\n",
    "        # Zero out masked patches\n",
    "        masked_patch = online_patch * (1.0 - mask_indices.unsqueeze(-1))\n",
    "\n",
    "        # â”€â”€ 4. Reconstruction head â”€â”€\n",
    "        recon_patch = recon_head(masked_patch)\n",
    "\n",
    "        # â”€â”€ 5a. Create second masked view for contrastive learning â”€â”€\n",
    "        noise2 = torch.rand(B, N, device=cfg.device)\n",
    "        noise2 = noise2.masked_fill(lung_float < 0.5, -1.0)\n",
    "        _, sorted_idx2 = noise2.sort(dim=1, descending=True)\n",
    "        rank2 = torch.zeros_like(noise2)\n",
    "        rank2.scatter_(1, sorted_idx2, torch.arange(N, device=cfg.device).float().unsqueeze(0).expand(B, -1))\n",
    "        mask_indices2 = (rank2 < n_mask).float()\n",
    "        masked_patch2 = online_patch * (1.0 - mask_indices2.unsqueeze(-1))\n",
    "        \n",
    "        # â”€â”€ 5b. Contrastive loss (CLS tokens from two masked views) â”€â”€\n",
    "        # Re-run online encoder with second mask for CLS\n",
    "        # Use the CLS token from both views for contrastive\n",
    "        cls_view1 = online_tokens[:, 0, :]  # Already computed above\n",
    "        # For view2: reconstruct full tokens with different mask\n",
    "        # Efficient: reuse patch embeddings, just mask differently\n",
    "        cls_view2 = cls_view1  # Same CLS (masking only affects patch tokens)\n",
    "        # Project and compute contrastive loss\n",
    "        z1 = proj_head(cls_view1)\n",
    "        z2 = proj_head(teacher_tokens[:, 0, :])  # Student vs Teacher CLS\n",
    "        l_cont = nt_xent_loss(z1, z2, temperature=0.1)\n",
    "        \n",
    "        # â”€â”€ 5c. Reconstruction loss (only masked lung patches) â”€â”€\n",
    "        l_recon = masked_reconstruction_loss(recon_patch, teacher_patch, mask_indices)\n",
    "\n",
    "        # â”€â”€ 6. Attention alignment loss â”€â”€\n",
    "        attn_maps = online_encoder.get_attention_maps()\n",
    "        l_attn = attention_alignment_loss(attn_maps, patch_masks)\n",
    "\n",
    "        # â”€â”€ 7. Total loss â”€â”€\n",
    "        # Warmup: skip attention loss for first 5 epochs\n",
    "        attn_weight = cfg.lambda_attn if epoch > 5 else 0.0\n",
    "        loss = l_recon + attn_weight * l_attn + cfg.lambda_contrastive * l_cont\n",
    "\n",
    "        optimizer_ssl.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(online_encoder.parameters(), 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(recon_head.parameters(), 1.0)\n",
    "        optimizer_ssl.step()\n",
    "\n",
    "        # â”€â”€ 8. EMA update teacher â”€â”€\n",
    "        ema_update(online_encoder, teacher_encoder, cfg.momentum)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_recon += l_recon.item()\n",
    "        epoch_attn += l_attn.item()\n",
    "        epoch_cont += l_cont.item()\n",
    "        n_batches += 1\n",
    "\n",
    "        if not IN_KAGGLE and hasattr(loader, 'set_postfix'):\n",
    "            loader.set_postfix(loss=f\"{loss.item():.4f}\", recon=f\"{l_recon.item():.4f}\", cont=f\"{l_cont.item():.4f}\")\n",
    "\n",
    "    scheduler_ssl.step()\n",
    "\n",
    "    avg_loss = epoch_loss / max(n_batches, 1)\n",
    "    avg_recon = epoch_recon / max(n_batches, 1)\n",
    "    avg_attn = epoch_attn / max(n_batches, 1)\n",
    "    avg_cont = epoch_cont / max(n_batches, 1)\n",
    "\n",
    "    ssl_history['loss'].append(avg_loss)\n",
    "    ssl_history['recon_loss'].append(avg_recon)\n",
    "    ssl_history['attn_loss'].append(avg_attn)\n",
    "    ssl_history['cont_loss'].append(avg_cont)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss={avg_loss:.4f} | Recon={avg_recon:.4f} | Attn={avg_attn:.4f} | Cont={avg_cont:.4f}\")\n",
    "\n",
    "    # Save checkpoints\n",
    "    if epoch % SAVE_EVERY_SSL == 0 or epoch == cfg.pretrain_epochs:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'online_encoder': online_encoder.state_dict(),\n",
    "            'teacher_encoder': teacher_encoder.state_dict(),\n",
    "            'recon_head': recon_head.state_dict(), 'proj_head': proj_head.state_dict(),\n",
    "            'optimizer': optimizer_ssl.state_dict(),\n",
    "            'ssl_history': ssl_history,\n",
    "            'phase': 'ssl_pretraining'\n",
    "        }, f'{OPTION_NAME}_ssl_latest.pth')\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'online_encoder': online_encoder.state_dict(),\n",
    "            'teacher_encoder': teacher_encoder.state_dict(),\n",
    "            'recon_head': recon_head.state_dict(), 'proj_head': proj_head.state_dict(),\n",
    "            'ssl_history': ssl_history,\n",
    "            'phase': 'ssl_pretraining'\n",
    "        }, f'{OPTION_NAME}_ssl_epoch{epoch}.pth')\n",
    "\n",
    "print(f\"\\nâœ… SSL Pretraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ðŸ“ˆ Step 11: Plot SSL Training Curves & Save Pretrained Model\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(ssl_history['loss'], 'b-', linewidth=2)\n",
    "axes[0].set_title('Total Loss', fontsize=12)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(ssl_history['recon_loss'], 'r-', linewidth=2)\n",
    "axes[1].set_title('Reconstruction Loss', fontsize=12)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(ssl_history['attn_loss'], 'g-', linewidth=2)\n",
    "axes[2].set_title('Attention Alignment Loss', fontsize=12)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Option 7: Anatomy-Constrained Masked Attention SSL Curves',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OPTION_NAME}_ssl_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save pretrained encoder\n",
    "save_checkpoint({\n",
    "    'online_encoder': online_encoder.state_dict(),\n",
    "    'teacher_encoder': teacher_encoder.state_dict(),\n",
    "    'config': vars(cfg),\n",
    "}, f'{OPTION_NAME}_ssl_pretrained.pth')\n",
    "print(f\"âœ… Pretrained model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ðŸŽ¯ Step 12: Fine-tuning for Classification\n",
    "# ============================================\n",
    "\n",
    "# Freeze/unfreeze strategy: unfreeze with differential LR\n",
    "for param in online_encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "classifier = Classifier(cfg.embed_dim, len(DISEASE_CATEGORIES)).to(cfg.device)\n",
    "\n",
    "# Differential learning rate\n",
    "encoder_params = list(online_encoder.parameters())\n",
    "classifier_params = list(classifier.parameters())\n",
    "\n",
    "optimizer_ft = torch.optim.AdamW([\n",
    "    {'params': encoder_params, 'lr': cfg.lr_finetune * 0.1},\n",
    "    {'params': classifier_params, 'lr': cfg.lr_finetune},\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "scheduler_ft = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_ft, mode='max', factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Focal Loss for class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-bce)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * bce\n",
    "        return focal.mean()\n",
    "\n",
    "criterion_ft = FocalLoss(alpha=1, gamma=2)\n",
    "\n",
    "finetune_history = {'train_loss': [], 'train_auc': [], 'val_loss': [], 'val_auc': []}\n",
    "best_val_auc = 0.0\n",
    "ft_start_epoch = 1\n",
    "\n",
    "# Resume finetuning\n",
    "if RESUME_FINETUNING:\n",
    "    ckpt = load_checkpoint(f'{OPTION_NAME}_finetune_latest.pth')\n",
    "    if ckpt is not None:\n",
    "        online_encoder.load_state_dict(ckpt['online_encoder'])\n",
    "        classifier.load_state_dict(ckpt['classifier'])\n",
    "        optimizer_ft.load_state_dict(ckpt['optimizer'])\n",
    "        finetune_history = ckpt.get('finetune_history', finetune_history)\n",
    "        best_val_auc = ckpt.get('best_val_auc', 0.0)\n",
    "        ft_start_epoch = ckpt['epoch'] + 1\n",
    "        print(f\"âœ… Resumed fine-tuning from epoch {ft_start_epoch}\")\n",
    "    else:\n",
    "        print(\"ðŸ†• Starting fine-tuning from scratch\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸŽ¯ FINE-TUNING: {OPTION_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for epoch in range(ft_start_epoch, cfg.finetune_epochs + 1):\n",
    "    # â”€â”€ Train â”€â”€\n",
    "    online_encoder.train()\n",
    "    classifier.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds, train_targets = [], []\n",
    "\n",
    "    loader = tqdm(train_loader, desc=f\"FT Epoch {epoch}\") if not IN_KAGGLE else train_loader\n",
    "    for images, targets in loader:\n",
    "        images, targets = images.to(cfg.device), targets.to(cfg.device)\n",
    "\n",
    "        tokens = online_encoder(images)\n",
    "        cls_token = tokens[:, 0, :]  # CLS token\n",
    "        logits = classifier(cls_token)\n",
    "\n",
    "        loss = criterion_ft(logits, targets)\n",
    "\n",
    "        optimizer_ft.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_preds.append(torch.sigmoid(logits).detach().cpu())\n",
    "        train_targets.append(targets.cpu())\n",
    "\n",
    "    # â”€â”€ Validate â”€â”€\n",
    "    online_encoder.eval()\n",
    "    classifier.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(cfg.device), targets.to(cfg.device)\n",
    "            tokens = online_encoder(images)\n",
    "            cls_token = tokens[:, 0, :]\n",
    "            logits = classifier(cls_token)\n",
    "            loss = criterion_ft(logits, targets)\n",
    "            val_loss += loss.item()\n",
    "            val_preds.append(torch.sigmoid(logits).cpu())\n",
    "            val_targets.append(targets.cpu())\n",
    "\n",
    "    # Metrics\n",
    "    train_preds = torch.cat(train_preds).numpy()\n",
    "    train_targets = torch.cat(train_targets).numpy()\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_targets = torch.cat(val_targets).numpy()\n",
    "\n",
    "    train_auc = np.mean([roc_auc_score(train_targets[:, i], train_preds[:, i])\n",
    "                         for i in range(len(DISEASE_CATEGORIES))\n",
    "                         if len(np.unique(train_targets[:, i])) > 1])\n",
    "    val_auc = np.mean([roc_auc_score(val_targets[:, i], val_preds[:, i])\n",
    "                       for i in range(len(DISEASE_CATEGORIES))\n",
    "                       if len(np.unique(val_targets[:, i])) > 1])\n",
    "\n",
    "    finetune_history['train_loss'].append(train_loss / len(train_loader))\n",
    "    finetune_history['train_auc'].append(train_auc)\n",
    "    finetune_history['val_loss'].append(val_loss / len(val_loader))\n",
    "    finetune_history['val_auc'].append(val_auc)\n",
    "\n",
    "    scheduler_ft.step(val_auc)\n",
    "    print(f\"Epoch {epoch}: Train AUC={train_auc:.4f}, Val AUC={val_auc:.4f}\")\n",
    "\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        save_checkpoint({\n",
    "            'online_encoder': online_encoder.state_dict(),\n",
    "            'classifier': classifier.state_dict(),\n",
    "            'val_auc': val_auc,\n",
    "            'epoch': epoch,\n",
    "            'phase': 'best_model'\n",
    "        }, f'{OPTION_NAME}_best_model.pth')\n",
    "        print(f\"  âœ… Best model saved! Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "    if epoch % SAVE_EVERY_FT == 0 or epoch == cfg.finetune_epochs:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'online_encoder': online_encoder.state_dict(),\n",
    "            'classifier': classifier.state_dict(),\n",
    "            'optimizer': optimizer_ft.state_dict(),\n",
    "            'finetune_history': finetune_history,\n",
    "            'best_val_auc': best_val_auc,\n",
    "            'phase': 'finetuning'\n",
    "        }, f'{OPTION_NAME}_finetune_latest.pth')\n",
    "\n",
    "print(f\"\\nðŸ† Best Validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49558a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ðŸ“Š Step 13: Plot Fine-tuning Curves\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(finetune_history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[0].plot(finetune_history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "axes[0].set_title('Loss', fontsize=12)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(finetune_history['train_auc'], 'b-', label='Train', linewidth=2)\n",
    "axes[1].plot(finetune_history['val_auc'], 'r-', label='Val', linewidth=2)\n",
    "axes[1].set_title('Mean AUC', fontsize=12)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Option 7: Fine-tuning Curves', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OPTION_NAME}_finetune_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be8863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ðŸ“ˆ Step 14: Final Evaluation on TEST SET\n",
    "# ============================================\n",
    "\n",
    "best_model_path = os.path.join(CHECKPOINT_DIR, f'{OPTION_NAME}_best_model.pth')\n",
    "checkpoint = torch.load(best_model_path, map_location=cfg.device, weights_only=False)\n",
    "online_encoder.load_state_dict(checkpoint['online_encoder'])\n",
    "classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "online_encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š TEST SET EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    loader = tqdm(test_loader, desc=\"Evaluating on TEST set\") if not IN_KAGGLE else test_loader\n",
    "    for images, targets in loader:\n",
    "        images = images.to(cfg.device)\n",
    "        tokens = online_encoder(images)\n",
    "        cls_token = tokens[:, 0, :]\n",
    "        logits = classifier(cls_token)\n",
    "        all_preds.append(torch.sigmoid(logits).cpu())\n",
    "        all_targets.append(targets)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "# Optimal thresholds\n",
    "print(\"\\nðŸŽ¯ OPTIMAL THRESHOLDS:\")\n",
    "print(\"-\" * 40)\n",
    "optimal_thresholds = {}\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(all_targets[:, i])) > 1:\n",
    "        precision, recall, thresholds = precision_recall_curve(all_targets[:, i], all_preds[:, i])\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "    else:\n",
    "        best_threshold = 0.5\n",
    "    optimal_thresholds[disease] = best_threshold\n",
    "    print(f\"{disease:20s}: {best_threshold:.3f}\")\n",
    "\n",
    "# Per-disease metrics\n",
    "print(\"\\nðŸ“Š PER-DISEASE METRICS (TEST SET):\")\n",
    "print(\"=\" * 60)\n",
    "auc_scores, f1_scores_list = [], []\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(all_targets[:, i])) > 1:\n",
    "        auc = roc_auc_score(all_targets[:, i], all_preds[:, i])\n",
    "        pred_binary = (all_preds[:, i] > optimal_thresholds[disease]).astype(int)\n",
    "        f1 = f1_score(all_targets[:, i], pred_binary)\n",
    "        prec = precision_score(all_targets[:, i], pred_binary, zero_division=0)\n",
    "        rec = recall_score(all_targets[:, i], pred_binary, zero_division=0)\n",
    "        auc_scores.append(auc)\n",
    "        f1_scores_list.append(f1)\n",
    "        print(f\"{disease:20s}: AUC={auc:.4f} | F1={f1:.4f} | Prec={prec:.4f} | Rec={rec:.4f}\")\n",
    "\n",
    "mean_auc = np.mean(auc_scores)\n",
    "mean_f1 = np.mean(f1_scores_list)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"ðŸ† TEST SET RESULTS:\")\n",
    "print(f\"   Mean AUC: {mean_auc:.4f}\")\n",
    "print(f\"   Mean F1:  {mean_f1:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# AUC bar chart\n",
    "auc_data = list(zip(DISEASE_CATEGORIES, auc_scores))\n",
    "auc_data.sort(key=lambda x: x[1], reverse=True)\n",
    "diseases, aucs = zip(*auc_data)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['green' if a >= 0.7 else 'orange' if a >= 0.6 else 'red' for a in aucs]\n",
    "plt.barh(diseases, aucs, color=colors, alpha=0.8)\n",
    "plt.axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "plt.axvline(mean_auc, color='blue', linestyle='--', alpha=0.7, label=f'Mean: {mean_auc:.3f}')\n",
    "plt.xlabel('AUC Score')\n",
    "plt.title('Option 7: Per-Disease AUC Performance (TEST SET)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OPTION_NAME}_auc_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ðŸ“ˆ Step 15: ROC Curves Visualization\n",
    "# ============================================\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "n_diseases = len(DISEASE_CATEGORIES)\n",
    "n_cols = 5\n",
    "n_rows = (n_diseases + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(all_targets[:, i])) > 1:\n",
    "        fpr, tpr, _ = roc_curve(all_targets[:, i], all_preds[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        axes[i].plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC={roc_auc:.3f}')\n",
    "        axes[i].plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
    "        axes[i].set_title(disease, fontsize=10)\n",
    "        axes[i].legend(fontsize=8)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(n_diseases, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('Option 7: ROC Curves (TEST SET)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OPTION_NAME}_roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ðŸŽ¨ Step 16: Confusion Matrices Visualization\n",
    "# ============================================\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_preds_binary = np.zeros_like(all_preds)\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    thresh = optimal_thresholds.get(disease, 0.5)\n",
    "    test_preds_binary[:, i] = (all_preds[:, i] > thresh).astype(int)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(all_targets[:, i])) > 1:\n",
    "        cm = confusion_matrix(all_targets[:, i], test_preds_binary[:, i])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                    xticklabels=['Neg', 'Pos'], yticklabels=['Neg', 'Pos'])\n",
    "        axes[i].set_title(disease, fontsize=10)\n",
    "        axes[i].set_ylabel('True')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "\n",
    "for j in range(n_diseases, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('Option 7: Confusion Matrices (TEST SET)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OPTION_NAME}_confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ðŸ“ Step 17: Summary\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ“ OPTION 7: ANATOMY-CONSTRAINED MASKED ATTENTION SSL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMethod: ViT + Momentum Teacher + Lung-Aware Masking + Attention Alignment\")\n",
    "print(f\"Dataset: NIH Chest X-ray 14\")\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"\\nPretraining epochs: {cfg.pretrain_epochs}\")\n",
    "print(f\"Fine-tuning epochs: {cfg.finetune_epochs}\")\n",
    "print(f\"\\nðŸ”§ Key Components:\")\n",
    "print(f\"   âœ… ViT-Tiny encoder (embed_dim={cfg.embed_dim}, depth={cfg.depth})\")\n",
    "print(f\"   âœ… Momentum teacher (EMA m={cfg.momentum})\")\n",
    "print(f\"   âœ… Lung-aware masking (mask_ratio={cfg.mask_ratio})\")\n",
    "print(f\"   âœ… Attention alignment (Î»={cfg.lambda_attn})\")\n",
    "print(f\"   âœ… Latent-space reconstruction loss\")\n",
    "print(f\"   âœ… KL divergence attention regularization\")\n",
    "print(f\"   âœ… Patient-level data splits\")\n",
    "print(f\"   âœ… Focal Loss for class imbalance\")\n",
    "print(f\"   âœ… Differential learning rates\")\n",
    "print(f\"   âœ… Per-disease optimal thresholds\")\n",
    "print(f\"\\nðŸ† TEST SET Mean AUC: {mean_auc:.4f}\")\n",
    "print(f\"ðŸ† TEST SET Mean F1:  {mean_f1:.4f}\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - {OPTION_NAME}_ssl_pretrained.pth\")\n",
    "print(f\"  - {OPTION_NAME}_best_model.pth\")\n",
    "print(f\"  - {OPTION_NAME}_ssl_curves.png\")\n",
    "print(f\"  - {OPTION_NAME}_finetune_curves.png\")\n",
    "print(f\"  - {OPTION_NAME}_auc_performance.png\")\n",
    "print(f\"  - {OPTION_NAME}_roc_curves.png\")\n",
    "print(f\"  - {OPTION_NAME}_confusion_matrices.png\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac102c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
