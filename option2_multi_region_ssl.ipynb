{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üì¶ Step 1: Import Libraries\n",
    "# ============================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OPENCV_LOG_LEVEL'] = 'SILENT'  # Suppress libpng ICC warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìÅ Step 2: Download and Load Dataset\n",
    "# ============================================\n",
    "\n",
    "path = kagglehub.dataset_download(\"khanfashee/nih-chest-x-ray-14-224x224-resized\")\n",
    "BASE_PATH = Path(path)\n",
    "print(f\"üìÇ Dataset path: {BASE_PATH}\")\n",
    "\n",
    "df = pd.read_csv(BASE_PATH / \"Data_Entry_2017.csv\")\n",
    "images_dir = BASE_PATH / \"images-224\" / \"images-224\"\n",
    "df[\"Image Path\"] = [str(images_dir / p) for p in df[\"Image Index\"].values]\n",
    "\n",
    "DISEASE_CATEGORIES = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass',\n",
    "    'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia'\n",
    "]\n",
    "\n",
    "for disease in DISEASE_CATEGORIES:\n",
    "    df[disease] = df['Finding Labels'].apply(lambda x: 1 if disease in x else 0)\n",
    "\n",
    "# Validate sample images\n",
    "sample_paths = df['Image Path'].sample(200, random_state=42).values\n",
    "missing = [p for p in sample_paths if not os.path.exists(p)]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"‚ùå Missing {len(missing)} images! First 3: {missing[:3]}\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df):,} images\")\n",
    "print(f\"üìä Disease categories: {len(DISEASE_CATEGORIES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ‚öôÔ∏è Step 3: Configuration\n",
    "# ============================================\n",
    "\n",
    "class Config:\n",
    "    # Model\n",
    "    img_size = 224\n",
    "    # Encoder backbone selection\n",
    "    encoder_backbone = 'custom'  # 'custom' or 'mobilenet_v2'\n",
    "\n",
    "    feat_dim = 256\n",
    "    proj_dim = 128\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 32  # Reduced from 64 to avoid OOM\n",
    "    pretrain_epochs = 50\n",
    "    finetune_epochs = 30\n",
    "    lr_pretrain = 1e-3\n",
    "    lr_finetune = 1e-4\n",
    "    temperature = 0.1\n",
    "    region_weight = 0.5  # Weight for region-specific loss\n",
    "    \n",
    "    # Data\n",
    "    num_workers = 4\n",
    "    use_subset = False\n",
    "    subset_size = 10000\n",
    "    \n",
    "    # Regions\n",
    "    num_vertical_regions = 3  # upper, middle, lower\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Memory optimization\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   Device: {cfg.device}\")\n",
    "print(f\"   Batch size: {cfg.batch_size}\")\n",
    "print(f\"   Num anatomical regions: {cfg.num_vertical_regions * 2 + 1}\")\n",
    "print(f\"   (Upper/Middle/Lower √ó Left/Right + Central)\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ‚öôÔ∏è Step 3: Configuration\n",
    "# ============================================\n",
    "\n",
    "class Config:\n",
    "    # Model\n",
    "    img_size = 224\n",
    "    # Encoder backbone selection\n",
    "    encoder_backbone = 'custom'  # 'custom' or 'mobilenet_v2'\n",
    "\n",
    "    feat_dim = 256\n",
    "    proj_dim = 128\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 32  # Reduced from 64 to avoid OOM\n",
    "    pretrain_epochs = 50\n",
    "    finetune_epochs = 30\n",
    "    lr_pretrain = 1e-3\n",
    "    lr_finetune = 1e-4\n",
    "    temperature = 0.1\n",
    "    region_weight = 0.5  # Weight for region-specific loss\n",
    "    \n",
    "    # Data\n",
    "    num_workers = 4\n",
    "    use_subset = False\n",
    "    subset_size = 10000\n",
    "    \n",
    "    # Regions\n",
    "    num_vertical_regions = 3  # upper, middle, lower\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Memory optimization\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   Device: {cfg.device}\")\n",
    "print(f\"   Batch size: {cfg.batch_size}\")\n",
    "print(f\"   Encoder backbone: {cfg.encoder_backbone}\")\n",
    "print(f\"   Num anatomical regions: {cfg.num_vertical_regions * 2 + 1}\")\n",
    "print(f\"   (Upper/Middle/Lower √ó Left/Right + Central)\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üó∫Ô∏è Step 4: Multi-Region Segmentation\n",
    "# ============================================\n",
    "\n",
    "def multi_region_segmentation(image, num_vertical_regions=3):\n",
    "    \"\"\"\n",
    "    Segment chest X-ray into anatomical regions\n",
    "    \n",
    "    Regions:\n",
    "    - Vertical: Upper, Middle, Lower lung fields\n",
    "    - Horizontal: Left, Right hemithorax\n",
    "    - Central: Mediastinum/heart area\n",
    "    \n",
    "    Args:\n",
    "        image: Grayscale image (H, W) or (1, H, W)\n",
    "        num_vertical_regions: Number of vertical divisions (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with region masks and metadata\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3 and image.shape[0] == 1:\n",
    "        image = image[0]\n",
    "    \n",
    "    h, w = image.shape\n",
    "    regions = {}\n",
    "    region_masks = {}\n",
    "    \n",
    "    # Vertical regions (upper/middle/lower lung fields)\n",
    "    region_height = h // num_vertical_regions\n",
    "    vert_names = ['upper', 'middle', 'lower']\n",
    "    \n",
    "    for i in range(num_vertical_regions):\n",
    "        mask = np.zeros_like(image)\n",
    "        start_h = i * region_height\n",
    "        end_h = h if i == num_vertical_regions - 1 else (i + 1) * region_height\n",
    "        mask[start_h:end_h, :] = 1.0\n",
    "        \n",
    "        region_name = f'vert_{vert_names[i]}'\n",
    "        regions[region_name] = image[start_h:end_h, :]\n",
    "        region_masks[region_name] = mask\n",
    "    \n",
    "    # Horizontal regions (left/right hemithorax)\n",
    "    left_mask = np.zeros_like(image)\n",
    "    right_mask = np.zeros_like(image)\n",
    "    left_mask[:, :w//2] = 1.0\n",
    "    right_mask[:, w//2:] = 1.0\n",
    "    \n",
    "    regions['horiz_left'] = image[:, :w//2]\n",
    "    regions['horiz_right'] = image[:, w//2:]\n",
    "    region_masks['horiz_left'] = left_mask\n",
    "    region_masks['horiz_right'] = right_mask\n",
    "    \n",
    "    # Central region (mediastinum/heart)\n",
    "    central_mask = np.zeros_like(image)\n",
    "    central_mask[h//3:2*h//3, w//3:2*w//3] = 1.0\n",
    "    regions['central_mediastinum'] = image[h//3:2*h//3, w//3:2*w//3]\n",
    "    region_masks['central_mediastinum'] = central_mask\n",
    "    \n",
    "    return {\n",
    "        'regions': regions,\n",
    "        'masks': region_masks,\n",
    "        'region_names': list(region_masks.keys())\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Multi-region segmentation function defined\")\n",
    "print(\"   6 anatomical regions: upper/middle/lower √ó left/right + mediastinum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1dc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üëÅÔ∏è Step 5: Visualize Region Segmentation\n",
    "# ============================================\n",
    "\n",
    "sample_indices = [0, 100, 500]\n",
    "fig = plt.figure(figsize=(16, 4*len(sample_indices)))\n",
    "\n",
    "color_map = {\n",
    "    'vert_upper': (1, 0, 0),      # Red\n",
    "    'vert_middle': (0, 1, 0),     # Green\n",
    "    'vert_lower': (0, 0, 1),      # Blue\n",
    "    'horiz_left': (1, 1, 0),      # Yellow\n",
    "    'horiz_right': (1, 0, 1),     # Magenta\n",
    "    'central_mediastinum': (0, 1, 1)  # Cyan\n",
    "}\n",
    "\n",
    "for row_idx, sample_idx in enumerate(sample_indices):\n",
    "    img_path = df.iloc[sample_idx]['Image Path']\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    img = img.resize((cfg.img_size, cfg.img_size), Image.LANCZOS)\n",
    "    img_np = np.array(img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    # Get region segmentation\n",
    "    seg_result = multi_region_segmentation(img_np)\n",
    "    \n",
    "    # Plot original\n",
    "    ax = plt.subplot(len(sample_indices), 2, row_idx*2 + 1)\n",
    "    ax.imshow(img_np, cmap='gray')\n",
    "    ax.set_title(f'Original Image {sample_idx}')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Plot regions with colors\n",
    "    ax = plt.subplot(len(sample_indices), 2, row_idx*2 + 2)\n",
    "    region_overlay = np.zeros((*img_np.shape, 3))\n",
    "    \n",
    "    for region_name, mask in seg_result['masks'].items():\n",
    "        color = color_map.get(region_name, (0.5, 0.5, 0.5))\n",
    "        for c in range(3):\n",
    "            region_overlay[:, :, c] += mask * color[c] * 0.4\n",
    "    \n",
    "    # Add original image\n",
    "    for c in range(3):\n",
    "        region_overlay[:, :, c] += img_np * 0.6\n",
    "    \n",
    "    region_overlay = np.clip(region_overlay, 0, 1)\n",
    "    ax.imshow(region_overlay)\n",
    "    ax.set_title(f'6 Anatomical Regions')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Option 2: Multi-Region Segmentation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('option2_region_segmentation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Region visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üîÑ Step 6: Augmentation\n",
    "# ============================================\n",
    "\n",
    "class ChestXrayAugment:\n",
    "    def __init__(self, img_size=224):\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, np.ndarray):\n",
    "            x = torch.tensor(img, dtype=torch.float32)\n",
    "        else:\n",
    "            x = img.clone()\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            x = torch.flip(x, dims=[2])\n",
    "        \n",
    "        if random.random() < 0.7:\n",
    "            angle = random.uniform(-15, 15)\n",
    "            x = transforms.functional.rotate(x, angle)\n",
    "        \n",
    "        if random.random() < 0.8:\n",
    "            factor = 1 + random.uniform(-0.2, 0.2)\n",
    "            x = transforms.functional.adjust_brightness(x, factor)\n",
    "        \n",
    "        if random.random() < 0.8:\n",
    "            factor = 1 + random.uniform(-0.2, 0.2)\n",
    "            x = transforms.functional.adjust_contrast(x, factor)\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            noise = torch.randn_like(x) * 0.05\n",
    "            x = torch.clamp(x + noise, 0, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "augment = ChestXrayAugment(cfg.img_size)\n",
    "print(\"‚úÖ Augmentation pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üì¶ Step 7: Dataset Classes\n",
    "# ============================================\n",
    "\n",
    "class MultiRegionPretrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, img_size=224, num_vertical_regions=3):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.num_vertical_regions = num_vertical_regions\n",
    "        \n",
    "        sample_paths = self.df['Image Path'].sample(min(200, len(self.df)), random_state=42).values\n",
    "        missing = [p for p in sample_paths if not os.path.exists(p)]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(f\"‚ùå Missing {len(missing)} images!\")\n",
    "        \n",
    "        print(f\"üì¶ MultiRegionPretrainDataset: {len(self.df)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['Image Path']\n",
    "        \n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img = img.resize((self.img_size, self.img_size), Image.LANCZOS)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        img = np.expand_dims(img, 0)\n",
    "        \n",
    "        # Get region masks\n",
    "        seg_result = multi_region_segmentation(img, self.num_vertical_regions)\n",
    "        region_masks = {}\n",
    "        for name, mask in seg_result['masks'].items():\n",
    "            region_masks[name] = torch.tensor(mask[None, ...], dtype=torch.float32)\n",
    "        \n",
    "        # Augmented views\n",
    "        if self.transform:\n",
    "            view1 = self.transform(img)\n",
    "            view2 = self.transform(img)\n",
    "        else:\n",
    "            view1 = torch.tensor(img, dtype=torch.float32)\n",
    "            view2 = torch.tensor(img, dtype=torch.float32)\n",
    "        \n",
    "        return view1, view2, region_masks\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    \"\"\"Classification dataset WITH augmentation support for fine-tuning\"\"\"\n",
    "    def __init__(self, df, disease_categories, img_size=224, is_training=False):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.disease_categories = disease_categories\n",
    "        self.img_size = img_size\n",
    "        self.is_training = is_training  # ‚úÖ Augmentation during fine-tuning!\n",
    "        print(f\"üì¶ ClassificationDataset: {len(self.df)} samples (training={is_training})\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['Image Path']).convert('L')\n",
    "        img = img.resize((self.img_size, self.img_size), Image.LANCZOS)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # ‚úÖ Apply augmentation during training (like DannyNet)\n",
    "        if self.is_training:\n",
    "            # Random horizontal flip\n",
    "            if np.random.random() > 0.5:\n",
    "                img = np.fliplr(img).copy()\n",
    "            # Random brightness\n",
    "            img = img * (0.8 + 0.4 * np.random.random())\n",
    "            # Random contrast\n",
    "            mean = img.mean()\n",
    "            img = (img - mean) * (0.8 + 0.4 * np.random.random()) + mean\n",
    "            # Random rotation (small)\n",
    "            if np.random.random() > 0.5:\n",
    "                angle = np.random.uniform(-10, 10)\n",
    "                img = rotate(img, angle, reshape=False, mode='constant', cval=0)\n",
    "            img = np.clip(img, 0, 1)\n",
    "        \n",
    "        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
    "        labels = torch.tensor([row[d] for d in self.disease_categories], dtype=torch.float32)\n",
    "        return img, labels\n",
    "\n",
    "print(\"‚úÖ Dataset classes defined (with training augmentation support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc35c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feat_dim=256, num_classes=14):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Initialize models with backbone selection\n",
    "if cfg.encoder_backbone == 'mobilenet_v2':\n",
    "    encoder = MobileNetV2Encoder(feat_dim=cfg.feat_dim, pretrained=True).to(cfg.device)\n",
    "    print(f\"‚úÖ Using MobileNetV2 encoder backbone\")\n",
    "else:\n",
    "    encoder = Encoder(feat_dim=cfg.feat_dim).to(cfg.device)\n",
    "    print(f\"‚úÖ Using custom CNN encoder backbone\")\n",
    "\n",
    "proj_head = ProjectionHead(cfg.feat_dim, cfg.proj_dim).to(cfg.device)\n",
    "decoder = Decoder(cfg.feat_dim, cfg.img_size).to(cfg.device)\n",
    "\n",
    "total_params = sum(p.numel() for m in [encoder, proj_head, decoder] for p in m.parameters())\n",
    "print(f\"‚úÖ Models initialized ({total_params:,} parameters)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ad6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üî• Step 9: Loss Functions\n",
    "# ============================================\n",
    "\n",
    "def nt_xent_loss(z1, z2, temperature=0.1):\n",
    "    device = z1.device\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    \n",
    "    batch_size = z1.shape[0]\n",
    "    representations = torch.cat([z1, z2], dim=0)\n",
    "    similarity = torch.matmul(representations, representations.T) / temperature\n",
    "    \n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool, device=device)\n",
    "    similarity = similarity.masked_fill(mask, -float('inf'))\n",
    "    \n",
    "    labels = torch.cat([torch.arange(batch_size) + batch_size,\n",
    "                        torch.arange(batch_size)]).to(device)\n",
    "    \n",
    "    return F.cross_entropy(similarity, labels)\n",
    "\n",
    "\n",
    "def region_aware_loss(proj_1, proj_2, region_masks_1, region_masks_2, \n",
    "                      temperature=0.1, region_weight=0.5):\n",
    "    \"\"\"\n",
    "    üó∫Ô∏è KEY INNOVATION: Region-aware contrastive loss\n",
    "    \n",
    "    - Standard NT-Xent loss as base\n",
    "    - Emphasizes samples with diverse anatomical region information\n",
    "    - Higher weight for multi-region pathology patterns\n",
    "    \"\"\"\n",
    "    # Base contrastive loss\n",
    "    base_loss = nt_xent_loss(proj_1, proj_2, temperature)\n",
    "    \n",
    "    # Calculate region coverage - how many regions have significant signal\n",
    "    all_masks = list(region_masks_1.values()) + list(region_masks_2.values())\n",
    "    region_scores = [m.mean() for m in all_masks]\n",
    "    avg_coverage = np.mean(region_scores)\n",
    "    \n",
    "    # Weight by anatomical completeness\n",
    "    weight_factor = 1.0 + region_weight * (avg_coverage - 0.5) * 2\n",
    "    \n",
    "    return base_loss * weight_factor\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚≠ê Focal Loss for handling class imbalance (from DannyNet SOTA)\n",
    "    FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Loss functions defined\")\n",
    "print(\"   üó∫Ô∏è region_aware_loss: Weights by anatomical region coverage\")\n",
    "print(\"   ‚≠ê FocalLoss: For class imbalance (Œ±=1.0, Œ≥=2.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìä Step 10: Create Data Loaders (Patient-Level Split)\n",
    "# ============================================\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL: Patient-level splitting to prevent data leakage\n",
    "# Same patient's images must stay in the same split\n",
    "print(\"=\"*60)\n",
    "print(\"üîÄ PATIENT-LEVEL SPLITTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "unique_patients = df['Patient ID'].unique()\n",
    "print(f\"Total unique patients: {len(unique_patients):,}\")\n",
    "\n",
    "# Split patients: 93% train, 5% val, 2% test\n",
    "train_val_patients, test_patients = train_test_split(\n",
    "    unique_patients, test_size=0.02, random_state=42\n",
    ")\n",
    "train_patients, val_patients = train_test_split(\n",
    "    train_val_patients, test_size=0.052, random_state=42  # ~5% of total\n",
    ")\n",
    "\n",
    "# Create dataframes based on patient splits\n",
    "train_df = df[df['Patient ID'].isin(train_patients)].copy()\n",
    "val_df = df[df['Patient ID'].isin(val_patients)].copy()\n",
    "test_df = df[df['Patient ID'].isin(test_patients)].copy()\n",
    "\n",
    "print(f\"‚úì Train: {len(train_df):,} images from {len(train_patients):,} patients\")\n",
    "print(f\"‚úì Val: {len(val_df):,} images from {len(val_patients):,} patients\")\n",
    "print(f\"‚úì Test: {len(test_df):,} images from {len(test_patients):,} patients\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if cfg.use_subset:\n",
    "    train_df = train_df.head(cfg.subset_size)\n",
    "    val_df = val_df.head(cfg.subset_size // 4)\n",
    "    test_df = test_df.head(cfg.subset_size // 8)\n",
    "    print(f\"‚ö° Using subset: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test\")\n",
    "\n",
    "# Datasets - NOW WITH AUGMENTATION FOR TRAINING\n",
    "train_pretrain_ds = MultiRegionPretrainDataset(train_df, transform=augment, img_size=cfg.img_size)\n",
    "train_class_ds = ClassificationDataset(train_df, DISEASE_CATEGORIES, cfg.img_size, is_training=True)  # ‚úÖ Augmentation ON\n",
    "val_class_ds = ClassificationDataset(val_df, DISEASE_CATEGORIES, cfg.img_size, is_training=False)\n",
    "test_class_ds = ClassificationDataset(test_df, DISEASE_CATEGORIES, cfg.img_size, is_training=False)\n",
    "\n",
    "# DataLoaders - FAST PIPELINE (like tf.data)\n",
    "# üöÄ num_workers: Parallel data loading (like num_parallel_calls)\n",
    "# üöÄ pin_memory: Faster CPU‚ÜíGPU transfer  \n",
    "# üöÄ prefetch_factor: Prefetch batches per worker (like prefetch)\n",
    "# üöÄ persistent_workers: Keep workers alive between epochs\n",
    "pretrain_loader = DataLoader(\n",
    "    train_pretrain_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=cfg.num_workers, pin_memory=True, drop_last=True,\n",
    "    prefetch_factor=2, persistent_workers=True if cfg.num_workers > 0 else False\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_class_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=cfg.num_workers, pin_memory=True, drop_last=True,\n",
    "    prefetch_factor=2, persistent_workers=True if cfg.num_workers > 0 else False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_class_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=cfg.num_workers, pin_memory=True,\n",
    "    prefetch_factor=2, persistent_workers=True if cfg.num_workers > 0 else False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_class_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=cfg.num_workers, pin_memory=True,\n",
    "    prefetch_factor=2, persistent_workers=True if cfg.num_workers > 0 else False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders ready - FAST PIPELINE (with training augmentation)\")\n",
    "print(f\"   Train batches: {len(pretrain_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590bcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üöÄ Step 11: Region-Aware SSL Pretraining\n",
    "# ============================================\n",
    "\n",
    "# Clear GPU cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "optimizer_ssl = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(proj_head.parameters()) + list(decoder.parameters()),\n",
    "    lr=cfg.lr_pretrain, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "ssl_history = {'loss': [], 'contrastive': [], 'reconstruction': []}\n",
    "START_EPOCH = 1\n",
    "\n",
    "if RESUME_SSL_PRETRAINING:\n",
    "    ckpt_file = find_latest_checkpoint(f'{OPTION_NAME}_ssl') if SSL_CHECKPOINT_FILE == \"latest\" else SSL_CHECKPOINT_FILE\n",
    "    if ckpt_file:\n",
    "        checkpoint = load_checkpoint(ckpt_file)\n",
    "        if checkpoint:\n",
    "            encoder.load_state_dict(checkpoint['encoder'])\n",
    "            proj_head.load_state_dict(checkpoint['proj_head'])\n",
    "            decoder.load_state_dict(checkpoint['decoder'])\n",
    "            if 'optimizer' in checkpoint:\n",
    "                optimizer_ssl.load_state_dict(checkpoint['optimizer'])\n",
    "            ssl_history = checkpoint.get('ssl_history', ssl_history)\n",
    "            START_EPOCH = checkpoint['epoch'] + 1\n",
    "            print(f\"üîÑ Resuming from epoch {START_EPOCH}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è RESUME_SSL_PRETRAINING=True but no checkpoint found. Starting fresh.\")\n",
    "\n",
    "if START_EPOCH > cfg.pretrain_epochs:\n",
    "    print(f\"‚úÖ SSL Pretraining already complete ({cfg.pretrain_epochs} epochs)\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Starting Option 2: Region-Aware SSL Pretraining\")\n",
    "    print(f\"   Epochs: {START_EPOCH} ‚Üí {cfg.pretrain_epochs}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    SAVE_EVERY = 1\n",
    "    \n",
    "    for epoch in range(START_EPOCH, cfg.pretrain_epochs + 1):\n",
    "        encoder.train()\n",
    "        proj_head.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_cont = 0\n",
    "        total_recon = 0\n",
    "        \n",
    "        loader = tqdm(pretrain_loader, desc=f\"Epoch {epoch}/{cfg.pretrain_epochs}\") if not IN_KAGGLE else pretrain_loader\n",
    "        for view1, view2, region_masks in loader:\n",
    "            view1 = view1.to(cfg.device)\n",
    "            view2 = view2.to(cfg.device)\n",
    "            region_masks = {k: v.to(cfg.device) for k, v in region_masks.items()}\n",
    "            \n",
    "            optimizer_ssl.zero_grad()\n",
    "            \n",
    "            z1 = encoder(view1)\n",
    "            z2 = encoder(view2)\n",
    "            \n",
    "            p1 = proj_head(z1)\n",
    "            p2 = proj_head(z2)\n",
    "            cont_loss = region_aware_loss(p1, p2, region_masks, region_masks, \n",
    "                                           cfg.temperature, cfg.region_weight)\n",
    "            \n",
    "            rec1 = decoder(z1)\n",
    "            rec2 = decoder(z2)\n",
    "            recon_loss = (F.mse_loss(rec1, view1) + F.mse_loss(rec2, view2)) / 2\n",
    "            \n",
    "            loss = cont_loss + 0.5 * recon_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_ssl.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_cont += cont_loss.item()\n",
    "            total_recon += recon_loss.item()\n",
    "            \n",
    "            if not IN_KAGGLE:\n",
    "                loader.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Free memory\n",
    "            del z1, z2, p1, p2, rec1, rec2, loss, cont_loss, recon_loss\n",
    "        \n",
    "        # Clear cache at end of epoch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        n = len(pretrain_loader)\n",
    "        ssl_history['loss'].append(total_loss / n)\n",
    "        ssl_history['contrastive'].append(total_cont / n)\n",
    "        ssl_history['reconstruction'].append(total_recon / n)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss={total_loss/n:.4f}, Cont={total_cont/n:.4f}, Recon={total_recon/n:.4f}\")\n",
    "        \n",
    "        if epoch % SAVE_EVERY == 0 or epoch == cfg.pretrain_epochs:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'proj_head': proj_head.state_dict(),\n",
    "                'decoder': decoder.state_dict(),\n",
    "                'optimizer': optimizer_ssl.state_dict(),\n",
    "                'ssl_history': ssl_history,\n",
    "            }, f'{OPTION_NAME}_ssl_latest.pth')\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'proj_head': proj_head.state_dict(),\n",
    "                'decoder': decoder.state_dict(),\n",
    "                'ssl_history': ssl_history,\n",
    "            }, f'{OPTION_NAME}_ssl_epoch{epoch}.pth')\n",
    "    \n",
    "    print(\"\\n‚úÖ Region-Aware SSL Pretraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìà Step 12: Plot SSL Training Curves\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(ssl_history['loss'], 'b-', linewidth=2)\n",
    "axes[0].set_title('Total Loss', fontsize=12)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(ssl_history['contrastive'], 'r-', linewidth=2)\n",
    "axes[1].set_title('Region-Aware Contrastive Loss', fontsize=12)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(ssl_history['reconstruction'], 'g-', linewidth=2)\n",
    "axes[2].set_title('Reconstruction Loss', fontsize=12)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Option 2: Region-Aware SSL Training', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('option2_ssl_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üíæ Step 13: Save Pretrained Model\n",
    "# ============================================\n",
    "\n",
    "torch.save({\n",
    "    'encoder': encoder.state_dict(),\n",
    "    'proj_head': proj_head.state_dict(),\n",
    "    'decoder': decoder.state_dict(),\n",
    "    'config': {'feat_dim': cfg.feat_dim, 'proj_dim': cfg.proj_dim}\n",
    "}, 'option2_ssl_pretrained.pth')\n",
    "\n",
    "print(\"üíæ Pretrained model saved: option2_ssl_pretrained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14241008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üéØ Step 14: Fine-tuning\n",
    "# ============================================\n",
    "# KEY IMPROVEMENTS (inspired by DannyNet SOTA):\n",
    "# 1. UNFREEZE encoder with differential learning rate\n",
    "# 2. Use Focal Loss instead of BCE (handles class imbalance better)\n",
    "# 3. Use AdamW optimizer (better generalization)\n",
    "# 4. More aggressive LR scheduler (factor=0.1, patience=2)\n",
    "# ============================================\n",
    "\n",
    "# ‚úÖ UNFREEZE encoder for fine-tuning (CRITICAL for performance!)\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = True  # UNFROZEN!\n",
    "encoder.train()\n",
    "\n",
    "classifier = Classifier(cfg.feat_dim, len(DISEASE_CATEGORIES)).to(cfg.device)\n",
    "\n",
    "# ‚úÖ Use Focal Loss (better for imbalanced multi-label classification)\n",
    "criterion = FocalLoss(alpha=1.0, gamma=2.0)\n",
    "\n",
    "# ‚úÖ Differential learning rates with AdamW\n",
    "encoder_lr = cfg.lr_finetune / 10  # 1e-5 if base is 1e-4\n",
    "classifier_lr = cfg.lr_finetune    # 1e-4\n",
    "\n",
    "optimizer_ft = torch.optim.AdamW([\n",
    "    {'params': encoder.parameters(), 'lr': encoder_lr},\n",
    "    {'params': classifier.parameters(), 'lr': classifier_lr}\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "# ‚úÖ More aggressive scheduler (like DannyNet)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_ft, mode='max', patience=2, factor=0.1, min_lr=1e-7\n",
    ")\n",
    "\n",
    "print(\"üîß Fine-tuning Configuration:\")\n",
    "print(f\"   ‚úÖ Encoder: UNFROZEN with LR={encoder_lr:.2e}\")\n",
    "print(f\"   ‚úÖ Classifier LR: {classifier_lr:.2e}\")\n",
    "print(f\"   ‚úÖ Loss: FocalLoss (Œ±=1.0, Œ≥=2.0)\")\n",
    "print(f\"   ‚úÖ Optimizer: AdamW\")\n",
    "print(f\"   ‚úÖ Scheduler: ReduceLROnPlateau (patience=2, factor=0.1)\")\n",
    "\n",
    "finetune_history = {'train_loss': [], 'train_auc': [], 'val_loss': [], 'val_auc': []}\n",
    "best_val_auc = 0\n",
    "FINETUNE_START_EPOCH = 1\n",
    "\n",
    "if RESUME_FINETUNING:\n",
    "    ckpt_file = find_latest_checkpoint(f'{OPTION_NAME}_finetune') if FINETUNE_CHECKPOINT_FILE == \"latest\" else FINETUNE_CHECKPOINT_FILE\n",
    "    if ckpt_file:\n",
    "        ft_checkpoint = load_checkpoint(ckpt_file)\n",
    "        if ft_checkpoint:\n",
    "            classifier.load_state_dict(ft_checkpoint['classifier'])\n",
    "            if 'encoder' in ft_checkpoint:\n",
    "                encoder.load_state_dict(ft_checkpoint['encoder'])\n",
    "            if 'optimizer' in ft_checkpoint:\n",
    "                try:\n",
    "                    optimizer_ft.load_state_dict(ft_checkpoint['optimizer'])\n",
    "                except:\n",
    "                    print(\"‚ö†Ô∏è Optimizer state incompatible, starting fresh\")\n",
    "            finetune_history = ft_checkpoint.get('finetune_history', finetune_history)\n",
    "            best_val_auc = ft_checkpoint.get('best_val_auc', 0)\n",
    "            FINETUNE_START_EPOCH = ft_checkpoint['epoch'] + 1\n",
    "            print(f\"üîÑ Resuming fine-tuning from epoch {FINETUNE_START_EPOCH}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è RESUME_FINETUNING=True but no checkpoint found. Starting fresh.\")\n",
    "\n",
    "if FINETUNE_START_EPOCH > cfg.finetune_epochs:\n",
    "    print(f\"‚úÖ Fine-tuning already complete ({cfg.finetune_epochs} epochs)\")\n",
    "else:\n",
    "    print(f\"\\nüéØ Starting Fine-tuning (ENCODER UNFROZEN)\")\n",
    "    print(f\"   Epochs: {FINETUNE_START_EPOCH} ‚Üí {cfg.finetune_epochs}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    SAVE_EVERY_FT = 5\n",
    "    \n",
    "    for epoch in range(FINETUNE_START_EPOCH, cfg.finetune_epochs + 1):\n",
    "        encoder.train()  # Encoder is also training now!\n",
    "        classifier.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "        \n",
    "        loader = tqdm(train_loader, desc=f\"Train {epoch}/{cfg.finetune_epochs}\") if not IN_KAGGLE else train_loader\n",
    "        for images, targets in loader:\n",
    "            images = images.to(cfg.device)\n",
    "            targets = targets.to(cfg.device)\n",
    "            \n",
    "            optimizer_ft.zero_grad()\n",
    "            \n",
    "            # ‚úÖ NO torch.no_grad() - encoder is being fine-tuned!\n",
    "            features = encoder(images)\n",
    "            logits = classifier(features)\n",
    "            loss = criterion(logits, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer_ft.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.append(torch.sigmoid(logits).detach().cpu())\n",
    "            train_targets.append(targets.cpu())\n",
    "        \n",
    "        encoder.eval()\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = images.to(cfg.device)\n",
    "                targets = targets.to(cfg.device)\n",
    "                \n",
    "                features = encoder(images)\n",
    "                logits = classifier(features)\n",
    "                loss = criterion(logits, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.append(torch.sigmoid(logits).cpu())\n",
    "                val_targets.append(targets.cpu())\n",
    "        \n",
    "        train_preds = torch.cat(train_preds).numpy()\n",
    "        train_targets = torch.cat(train_targets).numpy()\n",
    "        val_preds = torch.cat(val_preds).numpy()\n",
    "        val_targets = torch.cat(val_targets).numpy()\n",
    "        \n",
    "        train_auc = np.mean([roc_auc_score(train_targets[:, i], train_preds[:, i]) \n",
    "                             for i in range(len(DISEASE_CATEGORIES)) \n",
    "                             if len(np.unique(train_targets[:, i])) > 1])\n",
    "        val_auc = np.mean([roc_auc_score(val_targets[:, i], val_preds[:, i]) \n",
    "                           for i in range(len(DISEASE_CATEGORIES)) \n",
    "                           if len(np.unique(val_targets[:, i])) > 1])\n",
    "        \n",
    "        finetune_history['train_loss'].append(train_loss / len(train_loader))\n",
    "        finetune_history['train_auc'].append(train_auc)\n",
    "        finetune_history['val_loss'].append(val_loss / len(val_loader))\n",
    "        finetune_history['val_auc'].append(val_auc)\n",
    "        \n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        current_lr = optimizer_ft.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch}: Train AUC={train_auc:.4f}, Val AUC={val_auc:.4f}, LR={current_lr:.2e}\")\n",
    "        \n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            save_checkpoint({\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'classifier': classifier.state_dict(),\n",
    "                'val_auc': val_auc,\n",
    "                'epoch': epoch,\n",
    "            }, f'{OPTION_NAME}_best_model.pth')\n",
    "            print(f\"  ‚úÖ Best model saved! Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        if epoch % SAVE_EVERY_FT == 0 or epoch == cfg.finetune_epochs:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'classifier': classifier.state_dict(),\n",
    "                'optimizer': optimizer_ft.state_dict(),\n",
    "                'finetune_history': finetune_history,\n",
    "                'best_val_auc': best_val_auc,\n",
    "            }, f'{OPTION_NAME}_finetune_latest.pth')\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'classifier': classifier.state_dict(),\n",
    "                'finetune_history': finetune_history,\n",
    "                'best_val_auc': best_val_auc,\n",
    "            }, f'{OPTION_NAME}_finetune_epoch{epoch}.pth')\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìä Step 15: Final Evaluation on Validation Set\n",
    "# ============================================\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Load best model\n",
    "best_model_path = os.path.join(CHECKPOINT_DIR, f'{OPTION_NAME}_best_model.pth')\n",
    "checkpoint = torch.load(best_model_path, weights_only=False)\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    loader = tqdm(val_loader, desc=\"Evaluating Val\") if not IN_KAGGLE else val_loader\n",
    "    for images, targets in loader:\n",
    "        images = images.to(cfg.device)\n",
    "        features = encoder(images)\n",
    "        logits = classifier(features)\n",
    "        all_preds.append(torch.sigmoid(logits).cpu())\n",
    "        all_targets.append(targets)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "print(\"\\nüìä Validation Set - Per-Disease AUC Scores:\")\n",
    "print(\"=\" * 40)\n",
    "val_auc_scores = []\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(all_targets[:, i])) > 1:\n",
    "        auc = roc_auc_score(all_targets[:, i], all_preds[:, i])\n",
    "        val_auc_scores.append((disease, auc))\n",
    "        print(f\"{disease:20s}: {auc:.4f}\")\n",
    "\n",
    "val_mean_auc = np.mean([a for _, a in val_auc_scores])\n",
    "print(f\"\\n{'Val Mean AUC':20s}: {val_mean_auc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# üéØ Find Optimal Per-Disease Thresholds (like DannyNet)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ OPTIMAL THRESHOLD SEARCH (per-disease)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "optimal_thresholds = {}\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(all_targets[:, i])) > 1:\n",
    "        best_f1 = 0\n",
    "        best_thresh = 0.5\n",
    "        \n",
    "        # Search thresholds from 0.1 to 0.9\n",
    "        for thresh in np.arange(0.1, 0.9, 0.02):\n",
    "            preds_binary = (all_preds[:, i] > thresh).astype(int)\n",
    "            if preds_binary.sum() > 0 and (1 - preds_binary).sum() > 0:\n",
    "                f1 = f1_score(all_targets[:, i], preds_binary, zero_division=0)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_thresh = thresh\n",
    "        \n",
    "        optimal_thresholds[disease] = best_thresh\n",
    "        print(f\"{disease:20s}: optimal thresh = {best_thresh:.2f}, F1 = {best_f1:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# üß™ Step 16: Test Set Evaluation (Held-Out)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß™ TEST SET EVALUATION (Patient-Level Held-Out)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_preds, test_targets = [], []\n",
    "with torch.no_grad():\n",
    "    loader = tqdm(test_loader, desc=\"Evaluating Test\") if not IN_KAGGLE else test_loader\n",
    "    for images, targets in loader:\n",
    "        images = images.to(cfg.device)\n",
    "        features = encoder(images)\n",
    "        logits = classifier(features)\n",
    "        test_preds.append(torch.sigmoid(logits).cpu())\n",
    "        test_targets.append(targets)\n",
    "\n",
    "test_preds = torch.cat(test_preds).numpy()\n",
    "test_targets = torch.cat(test_targets).numpy()\n",
    "\n",
    "# Test AUC with fixed threshold (0.5)\n",
    "print(\"\\nüìä Test Set - Per-Disease AUC Scores (threshold=0.5):\")\n",
    "print(\"=\" * 40)\n",
    "test_auc_scores = []\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(test_targets[:, i])) > 1:\n",
    "        auc = roc_auc_score(test_targets[:, i], test_preds[:, i])\n",
    "        test_auc_scores.append((disease, auc))\n",
    "        print(f\"{disease:20s}: {auc:.4f}\")\n",
    "\n",
    "test_mean_auc = np.mean([a for _, a in test_auc_scores])\n",
    "print(f\"\\n{'Test Mean AUC':20s}: {test_mean_auc:.4f}\")\n",
    "\n",
    "# Test with OPTIMAL thresholds\n",
    "print(\"\\nüìä Test Set - With Optimal Thresholds (from validation):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Disease':20s} {'AUC':>8s} {'Thresh':>8s} {'F1':>8s} {'Precision':>10s} {'Recall':>8s}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_f1_scores = []\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(test_targets[:, i])) > 1:\n",
    "        auc = roc_auc_score(test_targets[:, i], test_preds[:, i])\n",
    "        thresh = optimal_thresholds.get(disease, 0.5)\n",
    "        preds_binary = (test_preds[:, i] > thresh).astype(int)\n",
    "        \n",
    "        f1 = f1_score(test_targets[:, i], preds_binary, zero_division=0)\n",
    "        prec = precision_score(test_targets[:, i], preds_binary, zero_division=0)\n",
    "        rec = recall_score(test_targets[:, i], preds_binary, zero_division=0)\n",
    "        \n",
    "        test_f1_scores.append(f1)\n",
    "        print(f\"{disease:20s} {auc:8.4f} {thresh:8.2f} {f1:8.4f} {prec:10.4f} {rec:8.4f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'MEAN':20s} {test_mean_auc:8.4f} {'--':>8s} {np.mean(test_f1_scores):8.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "test_auc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "diseases, aucs = zip(*test_auc_scores)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['green' if a >= 0.7 else 'orange' if a >= 0.6 else 'red' for a in aucs]\n",
    "plt.barh(diseases, aucs, color=colors, alpha=0.8)\n",
    "plt.axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "plt.axvline(test_mean_auc, color='blue', linestyle='--', alpha=0.7, label=f'Test Mean: {test_mean_auc:.3f}')\n",
    "plt.xlabel('AUC Score')\n",
    "plt.title('Option 2: Test Set Per-Disease AUC Performance', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('option2_test_auc_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# üìù Summary\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù OPTION 2: MULTI-REGION SEGMENTATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Method: Region-Aware SSL with 6 Anatomical Regions\")\n",
    "print(f\"Regions: Upper/Middle/Lower √ó Left/Right + Mediastinum\")\n",
    "print(f\"\\nDataset: NIH Chest X-ray 14 (Patient-Level Split)\")\n",
    "print(f\"  - Training: {len(train_df):,} images ({len(train_patients):,} patients)\")\n",
    "print(f\"  - Validation: {len(val_df):,} images ({len(val_patients):,} patients)\")\n",
    "print(f\"  - Test: {len(test_df):,} images ({len(test_patients):,} patients)\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  üìà Validation Mean AUC: {val_mean_auc:.4f}\")\n",
    "print(f\"  üß™ Test Mean AUC: {test_mean_auc:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
