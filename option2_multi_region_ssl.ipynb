{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üì¶ Step 1: Import Libraries\n",
    "# ============================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìÅ Step 2: Download and Load Dataset\n",
    "# ============================================\n",
    "\n",
    "path = kagglehub.dataset_download(\"khanfashee/nih-chest-x-ray-14-224x224-resized\")\n",
    "BASE_PATH = Path(path)\n",
    "print(f\"üìÇ Dataset path: {BASE_PATH}\")\n",
    "\n",
    "df_labels = pd.read_csv(BASE_PATH / \"Data_Entry_2017.csv\")\n",
    "images_dir = BASE_PATH / \"images-224\" / \"images-224\"\n",
    "df_labels[\"Image Path\"] = [str(images_dir / p) for p in df_labels[\"Image Index\"].values]\n",
    "\n",
    "DISEASE_CATEGORIES = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass',\n",
    "    'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia'\n",
    "]\n",
    "\n",
    "for disease in DISEASE_CATEGORIES:\n",
    "    df_labels[disease] = df_labels['Finding Labels'].apply(lambda x: 1 if disease in x else 0)\n",
    "\n",
    "# Validate sample images\n",
    "sample_paths = df_labels['Image Path'].sample(200, random_state=42).values\n",
    "missing = [p for p in sample_paths if not os.path.exists(p)]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"‚ùå Missing {len(missing)} images! First 3: {missing[:3]}\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_labels):,} images\")\n",
    "print(f\"üìä Disease categories: {len(DISEASE_CATEGORIES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ‚öôÔ∏è Step 3: Configuration\n",
    "# ============================================\n",
    "\n",
    "class Config:\n",
    "    # Model\n",
    "    img_size = 224\n",
    "    feat_dim = 256\n",
    "    proj_dim = 128\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 64\n",
    "    pretrain_epochs = 50\n",
    "    finetune_epochs = 30\n",
    "    lr_pretrain = 1e-3\n",
    "    lr_finetune = 1e-4\n",
    "    temperature = 0.1\n",
    "    region_weight = 0.5  # Weight for region-specific loss\n",
    "    \n",
    "    # Data\n",
    "    num_workers = 4\n",
    "    use_subset = False\n",
    "    subset_size = 10000\n",
    "    \n",
    "    # Regions\n",
    "    num_vertical_regions = 3  # upper, middle, lower\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   Device: {cfg.device}\")\n",
    "print(f\"   Num anatomical regions: {cfg.num_vertical_regions * 2 + 1}\")\n",
    "print(f\"   (Upper/Middle/Lower √ó Left/Right + Central)\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üíæ Step 3.5: Checkpoint & Resume Configuration\n",
    "# ============================================\n",
    "# ‚ö†Ô∏è EDIT THIS SECTION WHEN RESUMING AFTER DAYS/WEEKS\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë  üîß RESUME CONFIGURATION - EDIT THESE VALUES WHEN RESUMING  ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "OPTION_NAME = \"option2\"  # Used for checkpoint filenames\n",
    "\n",
    "# ===== STEP 1: Set your checkpoint dataset name =====\n",
    "CHECKPOINT_DATASET_NAME = \"chest-xray-ssl-checkpoints\"\n",
    "\n",
    "# ===== STEP 2: Set resume flags =====\n",
    "RESUME_SSL_PRETRAINING = False   # Set True to resume SSL pretraining\n",
    "RESUME_FINETUNING = False        # Set True to resume fine-tuning\n",
    "\n",
    "# ===== STEP 3: If resuming, specify which checkpoint to load =====\n",
    "SSL_CHECKPOINT_FILE = \"latest\"\n",
    "FINETUNE_CHECKPOINT_FILE = \"latest\"\n",
    "\n",
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                    END OF USER CONFIG                        ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "IN_KAGGLE = os.path.exists('/kaggle')\n",
    "IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/chest_xray_ssl'\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "    PREV_CHECKPOINT_DIR = f'/kaggle/input/{CHECKPOINT_DATASET_NAME}'\n",
    "    \n",
    "    if os.path.exists(PREV_CHECKPOINT_DIR):\n",
    "        print(f\"‚úÖ Found previous checkpoints at: {PREV_CHECKPOINT_DIR}\")\n",
    "        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "        for f in os.listdir(PREV_CHECKPOINT_DIR):\n",
    "            if f.endswith('.pth'):\n",
    "                src = os.path.join(PREV_CHECKPOINT_DIR, f)\n",
    "                dst = os.path.join(CHECKPOINT_DIR, f)\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.copy2(src, dst)\n",
    "                    print(f\"   üì¶ Copied: {f}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è No previous checkpoints at: {PREV_CHECKPOINT_DIR}\")\n",
    "elif not IN_COLAB:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    filepath = os.path.join(CHECKPOINT_DIR, filename)\n",
    "    state['saved_at'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    torch.save(state, filepath)\n",
    "    print(f\"üíæ Saved: {filename}\")\n",
    "    if IN_KAGGLE:\n",
    "        torch.save(state, f'/kaggle/working/{filename}')\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    filepath = os.path.join(CHECKPOINT_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=cfg.device)\n",
    "        print(f\"‚úÖ Loaded: {filename} (saved: {checkpoint.get('saved_at', 'Unknown')})\")\n",
    "        return checkpoint\n",
    "    return None\n",
    "\n",
    "def find_latest_checkpoint(prefix):\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        return None\n",
    "    latest_file = f'{prefix}_latest.pth'\n",
    "    if os.path.exists(os.path.join(CHECKPOINT_DIR, latest_file)):\n",
    "        return latest_file\n",
    "    import re\n",
    "    pattern = re.compile(rf'{prefix}_epoch(\\d+)\\.pth')\n",
    "    max_epoch, best_file = -1, None\n",
    "    for f in os.listdir(CHECKPOINT_DIR):\n",
    "        match = pattern.match(f)\n",
    "        if match and int(match.group(1)) > max_epoch:\n",
    "            max_epoch, best_file = int(match.group(1)), f\n",
    "    return best_file\n",
    "\n",
    "def list_checkpoints():\n",
    "    print(f\"\\nüìÅ Checkpoints in {CHECKPOINT_DIR}:\")\n",
    "    if os.path.exists(CHECKPOINT_DIR):\n",
    "        files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.endswith('.pth')])\n",
    "        for f in files:\n",
    "            size = os.path.getsize(os.path.join(CHECKPOINT_DIR, f)) / (1024*1024)\n",
    "            print(f\"   üì¶ {f} ({size:.1f}MB)\")\n",
    "        return files\n",
    "    return []\n",
    "\n",
    "print(f\"üîß Environment: {'Kaggle' if IN_KAGGLE else 'Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"üìÇ Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "list_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üó∫Ô∏è Step 4: Multi-Region Segmentation\n",
    "# ============================================\n",
    "\n",
    "def multi_region_segmentation(image, num_vertical_regions=3):\n",
    "    \"\"\"\n",
    "    Segment chest X-ray into anatomical regions\n",
    "    \n",
    "    Regions:\n",
    "    - Vertical: Upper, Middle, Lower lung fields\n",
    "    - Horizontal: Left, Right hemithorax\n",
    "    - Central: Mediastinum/heart area\n",
    "    \n",
    "    Args:\n",
    "        image: Grayscale image (H, W) or (1, H, W)\n",
    "        num_vertical_regions: Number of vertical divisions (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with region masks and metadata\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3 and image.shape[0] == 1:\n",
    "        image = image[0]\n",
    "    \n",
    "    h, w = image.shape\n",
    "    regions = {}\n",
    "    region_masks = {}\n",
    "    \n",
    "    # Vertical regions (upper/middle/lower lung fields)\n",
    "    region_height = h // num_vertical_regions\n",
    "    vert_names = ['upper', 'middle', 'lower']\n",
    "    \n",
    "    for i in range(num_vertical_regions):\n",
    "        mask = np.zeros_like(image)\n",
    "        start_h = i * region_height\n",
    "        end_h = h if i == num_vertical_regions - 1 else (i + 1) * region_height\n",
    "        mask[start_h:end_h, :] = 1.0\n",
    "        \n",
    "        region_name = f'vert_{vert_names[i]}'\n",
    "        regions[region_name] = image[start_h:end_h, :]\n",
    "        region_masks[region_name] = mask\n",
    "    \n",
    "    # Horizontal regions (left/right hemithorax)\n",
    "    left_mask = np.zeros_like(image)\n",
    "    right_mask = np.zeros_like(image)\n",
    "    left_mask[:, :w//2] = 1.0\n",
    "    right_mask[:, w//2:] = 1.0\n",
    "    \n",
    "    regions['horiz_left'] = image[:, :w//2]\n",
    "    regions['horiz_right'] = image[:, w//2:]\n",
    "    region_masks['horiz_left'] = left_mask\n",
    "    region_masks['horiz_right'] = right_mask\n",
    "    \n",
    "    # Central region (mediastinum/heart)\n",
    "    central_mask = np.zeros_like(image)\n",
    "    central_mask[h//3:2*h//3, w//3:2*w//3] = 1.0\n",
    "    regions['central_mediastinum'] = image[h//3:2*h//3, w//3:2*w//3]\n",
    "    region_masks['central_mediastinum'] = central_mask\n",
    "    \n",
    "    return {\n",
    "        'regions': regions,\n",
    "        'masks': region_masks,\n",
    "        'region_names': list(region_masks.keys())\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Multi-region segmentation function defined\")\n",
    "print(\"   6 anatomical regions: upper/middle/lower √ó left/right + mediastinum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1dc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üëÅÔ∏è Step 5: Visualize Region Segmentation\n",
    "# ============================================\n",
    "\n",
    "sample_indices = [0, 100, 500]\n",
    "fig = plt.figure(figsize=(16, 4*len(sample_indices)))\n",
    "\n",
    "color_map = {\n",
    "    'vert_upper': (1, 0, 0),      # Red\n",
    "    'vert_middle': (0, 1, 0),     # Green\n",
    "    'vert_lower': (0, 0, 1),      # Blue\n",
    "    'horiz_left': (1, 1, 0),      # Yellow\n",
    "    'horiz_right': (1, 0, 1),     # Magenta\n",
    "    'central_mediastinum': (0, 1, 1)  # Cyan\n",
    "}\n",
    "\n",
    "for row_idx, sample_idx in enumerate(sample_indices):\n",
    "    img_path = df_labels.iloc[sample_idx]['Image Path']\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    img = img.resize((cfg.img_size, cfg.img_size), Image.LANCZOS)\n",
    "    img_np = np.array(img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    # Get region segmentation\n",
    "    seg_result = multi_region_segmentation(img_np)\n",
    "    \n",
    "    # Plot original\n",
    "    ax = plt.subplot(len(sample_indices), 2, row_idx*2 + 1)\n",
    "    ax.imshow(img_np, cmap='gray')\n",
    "    ax.set_title(f'Original Image {sample_idx}')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Plot regions with colors\n",
    "    ax = plt.subplot(len(sample_indices), 2, row_idx*2 + 2)\n",
    "    region_overlay = np.zeros((*img_np.shape, 3))\n",
    "    \n",
    "    for region_name, mask in seg_result['masks'].items():\n",
    "        color = color_map.get(region_name, (0.5, 0.5, 0.5))\n",
    "        for c in range(3):\n",
    "            region_overlay[:, :, c] += mask * color[c] * 0.4\n",
    "    \n",
    "    # Add original image\n",
    "    for c in range(3):\n",
    "        region_overlay[:, :, c] += img_np * 0.6\n",
    "    \n",
    "    region_overlay = np.clip(region_overlay, 0, 1)\n",
    "    ax.imshow(region_overlay)\n",
    "    ax.set_title(f'6 Anatomical Regions')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Option 2: Multi-Region Segmentation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('option2_region_segmentation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Region visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üîÑ Step 6: Augmentation\n",
    "# ============================================\n",
    "\n",
    "class ChestXrayAugment:\n",
    "    def __init__(self, img_size=224):\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, np.ndarray):\n",
    "            x = torch.tensor(img, dtype=torch.float32)\n",
    "        else:\n",
    "            x = img.clone()\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            x = torch.flip(x, dims=[2])\n",
    "        \n",
    "        if random.random() < 0.7:\n",
    "            angle = random.uniform(-15, 15)\n",
    "            x = transforms.functional.rotate(x, angle)\n",
    "        \n",
    "        if random.random() < 0.8:\n",
    "            factor = 1 + random.uniform(-0.2, 0.2)\n",
    "            x = transforms.functional.adjust_brightness(x, factor)\n",
    "        \n",
    "        if random.random() < 0.8:\n",
    "            factor = 1 + random.uniform(-0.2, 0.2)\n",
    "            x = transforms.functional.adjust_contrast(x, factor)\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            noise = torch.randn_like(x) * 0.05\n",
    "            x = torch.clamp(x + noise, 0, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "augment = ChestXrayAugment(cfg.img_size)\n",
    "print(\"‚úÖ Augmentation pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üì¶ Step 7: Dataset Classes\n",
    "# ============================================\n",
    "\n",
    "class MultiRegionPretrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, img_size=224, num_vertical_regions=3):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.num_vertical_regions = num_vertical_regions\n",
    "        \n",
    "        sample_paths = self.df['Image Path'].sample(min(200, len(self.df)), random_state=42).values\n",
    "        missing = [p for p in sample_paths if not os.path.exists(p)]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(f\"‚ùå Missing {len(missing)} images!\")\n",
    "        \n",
    "        print(f\"üì¶ MultiRegionPretrainDataset: {len(self.df)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['Image Path']\n",
    "        \n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img = img.resize((self.img_size, self.img_size), Image.LANCZOS)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        img = np.expand_dims(img, 0)\n",
    "        \n",
    "        # Get region masks\n",
    "        seg_result = multi_region_segmentation(img, self.num_vertical_regions)\n",
    "        region_masks = {}\n",
    "        for name, mask in seg_result['masks'].items():\n",
    "            region_masks[name] = torch.tensor(mask[None, ...], dtype=torch.float32)\n",
    "        \n",
    "        # Augmented views\n",
    "        if self.transform:\n",
    "            view1 = self.transform(img)\n",
    "            view2 = self.transform(img)\n",
    "        else:\n",
    "            view1 = torch.tensor(img, dtype=torch.float32)\n",
    "            view2 = torch.tensor(img, dtype=torch.float32)\n",
    "        \n",
    "        return view1, view2, region_masks\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, df, disease_categories, img_size=224):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.disease_categories = disease_categories\n",
    "        self.img_size = img_size\n",
    "        print(f\"üì¶ ClassificationDataset: {len(self.df)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['Image Path']).convert('L')\n",
    "        img = img.resize((self.img_size, self.img_size), Image.LANCZOS)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        labels = torch.tensor([row[d] for d in self.disease_categories], dtype=torch.float32)\n",
    "        return img, labels\n",
    "\n",
    "print(\"‚úÖ Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc35c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üèóÔ∏è Step 8: Model Architecture\n",
    "# ============================================\n",
    "\n",
    "def conv_block(in_c, out_c, kernel=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel, stride, padding),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def residual_block(channels):\n",
    "    return nn.Sequential(\n",
    "        conv_block(channels, channels),\n",
    "        conv_block(channels, channels)\n",
    "    )\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, feat_dim=256):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(in_channels, 64), residual_block(64), nn.MaxPool2d(2),\n",
    "            conv_block(64, 128), residual_block(128), nn.MaxPool2d(2),\n",
    "            conv_block(128, 256), residual_block(256), residual_block(256), nn.MaxPool2d(2),\n",
    "            conv_block(256, 512), residual_block(512), residual_block(512), nn.MaxPool2d(2),\n",
    "            conv_block(512, 512), residual_block(512), nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, feat_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, feat_dim=256, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_dim, feat_dim), nn.BatchNorm1d(feat_dim), nn.ReLU(),\n",
    "            nn.Linear(feat_dim, proj_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, feat_dim=256, img_size=224):\n",
    "        super().__init__()\n",
    "        self.init_size = img_size // 32\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256 * self.init_size * self.init_size), nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 4, 2, 1), nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(z.size(0), 256, self.init_size, self.init_size)\n",
    "        return self.decoder(x)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feat_dim=256, num_classes=14):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "encoder = Encoder(feat_dim=cfg.feat_dim).to(cfg.device)\n",
    "proj_head = ProjectionHead(cfg.feat_dim, cfg.proj_dim).to(cfg.device)\n",
    "decoder = Decoder(cfg.feat_dim, cfg.img_size).to(cfg.device)\n",
    "\n",
    "total_params = sum(p.numel() for m in [encoder, proj_head, decoder] for p in m.parameters())\n",
    "print(f\"‚úÖ Models initialized ({total_params:,} parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ad6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üî• Step 9: Loss Functions\n",
    "# ============================================\n",
    "\n",
    "def nt_xent_loss(z1, z2, temperature=0.1):\n",
    "    device = z1.device\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    \n",
    "    batch_size = z1.shape[0]\n",
    "    representations = torch.cat([z1, z2], dim=0)\n",
    "    similarity = torch.matmul(representations, representations.T) / temperature\n",
    "    \n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool, device=device)\n",
    "    similarity = similarity.masked_fill(mask, -float('inf'))\n",
    "    \n",
    "    labels = torch.cat([torch.arange(batch_size) + batch_size,\n",
    "                        torch.arange(batch_size)]).to(device)\n",
    "    \n",
    "    return F.cross_entropy(similarity, labels)\n",
    "\n",
    "\n",
    "def region_aware_loss(proj_1, proj_2, region_masks_1, region_masks_2, \n",
    "                      temperature=0.1, region_weight=0.5):\n",
    "    \"\"\"\n",
    "    üó∫Ô∏è KEY INNOVATION: Region-aware contrastive loss\n",
    "    \n",
    "    - Standard NT-Xent loss as base\n",
    "    - Emphasizes samples with diverse anatomical region information\n",
    "    - Higher weight for multi-region pathology patterns\n",
    "    \"\"\"\n",
    "    # Base contrastive loss\n",
    "    base_loss = nt_xent_loss(proj_1, proj_2, temperature)\n",
    "    \n",
    "    # Calculate region coverage - how many regions have significant signal\n",
    "    all_masks = list(region_masks_1.values()) + list(region_masks_2.values())\n",
    "    region_scores = [m.mean() for m in all_masks]\n",
    "    avg_coverage = np.mean(region_scores)\n",
    "    \n",
    "    # Weight by anatomical completeness\n",
    "    weight_factor = 1.0 + region_weight * (avg_coverage - 0.5) * 2\n",
    "    \n",
    "    return base_loss * weight_factor\n",
    "\n",
    "print(\"‚úÖ Loss functions defined\")\n",
    "print(\"   üó∫Ô∏è region_aware_loss: Weights by anatomical region coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìä Step 10: Create Data Loaders\n",
    "# ============================================\n",
    "\n",
    "df_shuffled = df_labels.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_size = int(0.8 * len(df_shuffled))\n",
    "train_df = df_shuffled[:train_size]\n",
    "val_df = df_shuffled[train_size:]\n",
    "\n",
    "if cfg.use_subset:\n",
    "    train_df = train_df.head(cfg.subset_size)\n",
    "    val_df = val_df.head(cfg.subset_size // 4)\n",
    "\n",
    "train_pretrain_ds = MultiRegionPretrainDataset(train_df, transform=augment, img_size=cfg.img_size)\n",
    "train_class_ds = ClassificationDataset(train_df, DISEASE_CATEGORIES, cfg.img_size)\n",
    "val_class_ds = ClassificationDataset(val_df, DISEASE_CATEGORIES, cfg.img_size)\n",
    "\n",
    "pretrain_loader = DataLoader(\n",
    "    train_pretrain_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=cfg.num_workers, pin_memory=True, drop_last=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_class_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=cfg.num_workers, pin_memory=True, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_class_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=cfg.num_workers, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590bcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üöÄ Step 11: Region-Aware SSL Pretraining\n",
    "# ============================================\n",
    "\n",
    "optimizer_ssl = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(proj_head.parameters()) + list(decoder.parameters()),\n",
    "    lr=cfg.lr_pretrain, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "ssl_history = {'loss': [], 'contrastive': [], 'reconstruction': []}\n",
    "START_EPOCH = 1\n",
    "\n",
    "# ===== AUTO-RESUME FROM CHECKPOINT =====\n",
    "if RESUME_SSL_PRETRAINING:\n",
    "    ckpt_file = find_latest_checkpoint(f'{OPTION_NAME}_ssl') if SSL_CHECKPOINT_FILE == \"latest\" else SSL_CHECKPOINT_FILE\n",
    "    if ckpt_file:\n",
    "        checkpoint = load_checkpoint(ckpt_file)\n",
    "        if checkpoint:\n",
    "            encoder.load_state_dict(checkpoint['encoder'])\n",
    "            proj_head.load_state_dict(checkpoint['proj_head'])\n",
    "            decoder.load_state_dict(checkpoint['decoder'])\n",
    "            if 'optimizer' in checkpoint:\n",
    "                optimizer_ssl.load_state_dict(checkpoint['optimizer'])\n",
    "            ssl_history = checkpoint.get('ssl_history', ssl_history)\n",
    "            START_EPOCH = checkpoint['epoch'] + 1\n",
    "            print(f\"üîÑ Resuming from epoch {START_EPOCH}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è RESUME_SSL_PRETRAINING=True but no checkpoint found. Starting fresh.\")\n",
    "\n",
    "if START_EPOCH > cfg.pretrain_epochs:\n",
    "    print(f\"‚úÖ SSL Pretraining already complete ({cfg.pretrain_epochs} epochs)\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Starting Option 2: Region-Aware SSL Pretraining\")\n",
    "    print(f\"   Epochs: {START_EPOCH} ‚Üí {cfg.pretrain_epochs}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    SAVE_EVERY = 1\n",
    "    \n",
    "    for epoch in range(START_EPOCH, cfg.pretrain_epochs + 1):\n",
    "        encoder.train()\n",
    "        proj_head.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_cont = 0\n",
    "        total_recon = 0\n",
    "        \n",
    "        pbar = tqdm(pretrain_loader, desc=f\"Epoch {epoch}/{cfg.pretrain_epochs}\")\n",
    "        for view1, view2, region_masks in pbar:\n",
    "            view1 = view1.to(cfg.device)\n",
    "            view2 = view2.to(cfg.device)\n",
    "            region_masks = {k: v.to(cfg.device) for k, v in region_masks.items()}\n",
    "            \n",
    "            optimizer_ssl.zero_grad()\n",
    "            \n",
    "            z1 = encoder(view1)\n",
    "            z2 = encoder(view2)\n",
    "            \n",
    "            p1 = proj_head(z1)\n",
    "            p2 = proj_head(z2)\n",
    "            cont_loss = region_aware_loss(p1, p2, region_masks, region_masks, \n",
    "                                           cfg.temperature, cfg.region_weight)\n",
    "            \n",
    "            rec1 = decoder(z1)\n",
    "            rec2 = decoder(z2)\n",
    "            recon_loss = (F.mse_loss(rec1, view1) + F.mse_loss(rec2, view2)) / 2\n",
    "            \n",
    "            loss = cont_loss + 0.5 * recon_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_ssl.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_cont += cont_loss.item()\n",
    "            total_recon += recon_loss.item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        n = len(pretrain_loader)\n",
    "        ssl_history['loss'].append(total_loss / n)\n",
    "        ssl_history['contrastive'].append(total_cont / n)\n",
    "        ssl_history['reconstruction'].append(total_recon / n)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss={total_loss/n:.4f}, Cont={total_cont/n:.4f}, Recon={total_recon/n:.4f}\")\n",
    "        \n",
    "        # ===== SAVE CHECKPOINT =====\n",
    "        if epoch % SAVE_EVERY == 0 or epoch == cfg.pretrain_epochs:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'proj_head': proj_head.state_dict(),\n",
    "                'decoder': decoder.state_dict(),\n",
    "                'optimizer': optimizer_ssl.state_dict(),\n",
    "                'ssl_history': ssl_history,\n",
    "            }, f'{OPTION_NAME}_ssl_latest.pth')\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'proj_head': proj_head.state_dict(),\n",
    "                'decoder': decoder.state_dict(),\n",
    "                'ssl_history': ssl_history,\n",
    "            }, f'{OPTION_NAME}_ssl_epoch{epoch}.pth')\n",
    "    \n",
    "    print(\"\\n‚úÖ Region-Aware SSL Pretraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìà Step 12: Plot SSL Training Curves\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(ssl_history['loss'], 'b-', linewidth=2)\n",
    "axes[0].set_title('Total Loss', fontsize=12)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(ssl_history['contrastive'], 'r-', linewidth=2)\n",
    "axes[1].set_title('Region-Aware Contrastive Loss', fontsize=12)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(ssl_history['reconstruction'], 'g-', linewidth=2)\n",
    "axes[2].set_title('Reconstruction Loss', fontsize=12)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Option 2: Region-Aware SSL Training', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('option2_ssl_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üíæ Step 13: Save Pretrained Model\n",
    "# ============================================\n",
    "\n",
    "torch.save({\n",
    "    'encoder': encoder.state_dict(),\n",
    "    'proj_head': proj_head.state_dict(),\n",
    "    'decoder': decoder.state_dict(),\n",
    "    'config': {'feat_dim': cfg.feat_dim, 'proj_dim': cfg.proj_dim}\n",
    "}, 'option2_ssl_pretrained.pth')\n",
    "\n",
    "print(\"üíæ Pretrained model saved: option2_ssl_pretrained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14241008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üéØ Step 14: Fine-tuning\n",
    "# ============================================\n",
    "\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "encoder.eval()\n",
    "\n",
    "classifier = Classifier(cfg.feat_dim, len(DISEASE_CATEGORIES)).to(cfg.device)\n",
    "\n",
    "pos_counts = train_df[DISEASE_CATEGORIES].sum().values\n",
    "neg_counts = len(train_df) - pos_counts\n",
    "pos_weights = torch.tensor(neg_counts / (pos_counts + 1e-6), dtype=torch.float32).to(cfg.device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "optimizer_ft = torch.optim.Adam(classifier.parameters(), lr=cfg.lr_finetune, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'max', patience=5, factor=0.5)\n",
    "\n",
    "finetune_history = {'train_loss': [], 'train_auc': [], 'val_loss': [], 'val_auc': []}\n",
    "best_val_auc = 0\n",
    "FINETUNE_START_EPOCH = 1\n",
    "\n",
    "# ===== AUTO-RESUME FINE-TUNING =====\n",
    "if RESUME_FINETUNING:\n",
    "    ckpt_file = find_latest_checkpoint(f'{OPTION_NAME}_finetune') if FINETUNE_CHECKPOINT_FILE == \"latest\" else FINETUNE_CHECKPOINT_FILE\n",
    "    if ckpt_file:\n",
    "        ft_checkpoint = load_checkpoint(ckpt_file)\n",
    "        if ft_checkpoint:\n",
    "            classifier.load_state_dict(ft_checkpoint['classifier'])\n",
    "            if 'optimizer' in ft_checkpoint:\n",
    "                optimizer_ft.load_state_dict(ft_checkpoint['optimizer'])\n",
    "            finetune_history = ft_checkpoint.get('finetune_history', finetune_history)\n",
    "            best_val_auc = ft_checkpoint.get('best_val_auc', 0)\n",
    "            FINETUNE_START_EPOCH = ft_checkpoint['epoch'] + 1\n",
    "            print(f\"üîÑ Resuming fine-tuning from epoch {FINETUNE_START_EPOCH}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è RESUME_FINETUNING=True but no checkpoint found. Starting fresh.\")\n",
    "\n",
    "if FINETUNE_START_EPOCH > cfg.finetune_epochs:\n",
    "    print(f\"‚úÖ Fine-tuning already complete ({cfg.finetune_epochs} epochs)\")\n",
    "else:\n",
    "    print(f\"\\nüéØ Starting Fine-tuning: Epochs {FINETUNE_START_EPOCH} ‚Üí {cfg.finetune_epochs}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    SAVE_EVERY_FT = 5\n",
    "    \n",
    "    for epoch in range(FINETUNE_START_EPOCH, cfg.finetune_epochs + 1):\n",
    "        classifier.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "        \n",
    "        for images, targets in tqdm(train_loader, desc=f\"Train {epoch}/{cfg.finetune_epochs}\"):\n",
    "            images = images.to(cfg.device)\n",
    "            targets = targets.to(cfg.device)\n",
    "            \n",
    "            optimizer_ft.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                features = encoder(images)\n",
    "            logits = classifier(features)\n",
    "            loss = criterion(logits, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_ft.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.append(torch.sigmoid(logits).detach().cpu())\n",
    "            train_targets.append(targets.cpu())\n",
    "        \n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = images.to(cfg.device)\n",
    "                targets = targets.to(cfg.device)\n",
    "                \n",
    "                features = encoder(images)\n",
    "                logits = classifier(features)\n",
    "                loss = criterion(logits, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.append(torch.sigmoid(logits).cpu())\n",
    "                val_targets.append(targets.cpu())\n",
    "        \n",
    "        train_preds = torch.cat(train_preds).numpy()\n",
    "        train_targets = torch.cat(train_targets).numpy()\n",
    "        val_preds = torch.cat(val_preds).numpy()\n",
    "        val_targets = torch.cat(val_targets).numpy()\n",
    "        \n",
    "        train_auc = np.mean([roc_auc_score(train_targets[:, i], train_preds[:, i]) \n",
    "                             for i in range(len(DISEASE_CATEGORIES)) \n",
    "                             if len(np.unique(train_targets[:, i])) > 1])\n",
    "        val_auc = np.mean([roc_auc_score(val_targets[:, i], val_preds[:, i]) \n",
    "                           for i in range(len(DISEASE_CATEGORIES)) \n",
    "                           if len(np.unique(val_targets[:, i])) > 1])\n",
    "        \n",
    "        finetune_history['train_loss'].append(train_loss / len(train_loader))\n",
    "        finetune_history['train_auc'].append(train_auc)\n",
    "        finetune_history['val_loss'].append(val_loss / len(val_loader))\n",
    "        finetune_history['val_auc'].append(val_auc)\n",
    "        \n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train AUC={train_auc:.4f}, Val AUC={val_auc:.4f}\")\n",
    "        \n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            save_checkpoint({\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'classifier': classifier.state_dict(),\n",
    "                'val_auc': val_auc,\n",
    "                'epoch': epoch,\n",
    "            }, f'{OPTION_NAME}_best_model.pth')\n",
    "            print(f\"  ‚úÖ Best model saved! Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        if epoch % SAVE_EVERY_FT == 0 or epoch == cfg.finetune_epochs:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'classifier': classifier.state_dict(),\n",
    "                'optimizer': optimizer_ft.state_dict(),\n",
    "                'finetune_history': finetune_history,\n",
    "                'best_val_auc': best_val_auc,\n",
    "            }, f'{OPTION_NAME}_finetune_latest.pth')\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìä Step 15: Final Evaluation & Summary\n",
    "# ============================================\n",
    "\n",
    "checkpoint = torch.load('option2_best_model.pth')\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(cfg.device)\n",
    "        features = encoder(images)\n",
    "        logits = classifier(features)\n",
    "        all_preds.append(torch.sigmoid(logits).cpu())\n",
    "        all_targets.append(targets)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "print(\"\\nüìä Per-Disease AUC Scores:\")\n",
    "print(\"=\" * 40)\n",
    "auc_scores = []\n",
    "for i, disease in enumerate(DISEASE_CATEGORIES):\n",
    "    if len(np.unique(all_targets[:, i])) > 1:\n",
    "        auc = roc_auc_score(all_targets[:, i], all_preds[:, i])\n",
    "        auc_scores.append((disease, auc))\n",
    "        print(f\"{disease:20s}: {auc:.4f}\")\n",
    "\n",
    "mean_auc = np.mean([a for _, a in auc_scores])\n",
    "print(f\"\\n{'Mean AUC':20s}: {mean_auc:.4f}\")\n",
    "\n",
    "auc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "diseases, aucs = zip(*auc_scores)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['green' if a >= 0.7 else 'orange' if a >= 0.6 else 'red' for a in aucs]\n",
    "plt.barh(diseases, aucs, color=colors, alpha=0.8)\n",
    "plt.axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "plt.axvline(mean_auc, color='blue', linestyle='--', alpha=0.7, label=f'Mean: {mean_auc:.3f}')\n",
    "plt.xlabel('AUC Score')\n",
    "plt.title('Option 2: Per-Disease AUC Performance', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('option2_auc_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù OPTION 2: MULTI-REGION SEGMENTATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Method: Region-Aware SSL with 6 Anatomical Regions\")\n",
    "print(f\"Regions: Upper/Middle/Lower √ó Left/Right + Mediastinum\")\n",
    "print(f\"\\nüèÜ Final Mean AUC: {mean_auc:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
